[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "",
    "text": "Ce projet porte sur l’analyse statistique des profils GitHub les plus influents à travers le monde. GitHub étant la plateforme de référence pour le développement collaboratif, le nombre d’étoiles (stars) récoltées par un utilisateur est devenu un indicateur clé de sa popularité et de son impact dans la communauté “Open Source”.\nL’objectif de cette étude est de comprendre les caractéristiques de ces profils d’exception. À l’aide d’un jeu de données réel regroupant 30 développeurs et organisations majeures (tels que Google, Microsoft ou OpenAI), nous tenterons de répondre à plusieurs questions fondamentales :\n\nComment la popularité est-elle distribuée entre ces différents profils ?\nLe choix des langages de programmation (comme Python ou JavaScript) influence-t-il le succès d’un profil ?\nExiste-t-il des disparités géographiques ou des différences notables entre les comptes individuels et les organisations ?"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "",
    "text": "Ce projet porte sur l’analyse statistique des profils GitHub les plus influents à travers le monde. GitHub étant la plateforme de référence pour le développement collaboratif, le nombre d’étoiles (stars) récoltées par un utilisateur est devenu un indicateur clé de sa popularité et de son impact dans la communauté “Open Source”.\nL’objectif de cette étude est de comprendre les caractéristiques de ces profils d’exception. À l’aide d’un jeu de données réel regroupant 30 développeurs et organisations majeures (tels que Google, Microsoft ou OpenAI), nous tenterons de répondre à plusieurs questions fondamentales :\n\nComment la popularité est-elle distribuée entre ces différents profils ?\nLe choix des langages de programmation (comme Python ou JavaScript) influence-t-il le succès d’un profil ?\nExiste-t-il des disparités géographiques ou des différences notables entre les comptes individuels et les organisations ?"
  },
  {
    "objectID": "index.html#status",
    "href": "index.html#status",
    "title": "Statistics with R project– Template",
    "section": "2 Status",
    "text": "2 Status\nIn progress"
  },
  {
    "objectID": "index.html#related-pages",
    "href": "index.html#related-pages",
    "title": "Statistics with R project– Template",
    "section": "3 Related Pages",
    "text": "3 Related Pages\n\nLecture slides\nDatasets\nSource code of the exercises"
  },
  {
    "objectID": "index.html#author",
    "href": "index.html#author",
    "title": "Statistics with R project– Template",
    "section": "4 Author",
    "text": "4 Author\nAbdallah Khemais"
  },
  {
    "objectID": "qmd/15-multivariate-small-streams.html",
    "href": "qmd/15-multivariate-small-streams.html",
    "title": "15-Multivariate Analysis of Macrozoobenthos Samples from Two Small Streams",
    "section": "",
    "text": "The aim of the following exercise is to demonstrate some important multivariate methods by example of a macrozoobenthos data set from two small streams. In some cases, several alternatives are presented, but for a real analysis one does not need to include everything. On the other hand, the methods offer further possibilities that cannot all be presented, see the online help and corresponding books and tutorials.\nThe data set used originates from a field experiment to investigate the influence of fish predation on the macrozoobenthos species composition. However, during the study period an extreme flood occurred in August and caused major morphological changes to the streams. This motivated the hypothesis that the species community has also changed."
  },
  {
    "objectID": "qmd/15-multivariate-small-streams.html#nmds",
    "href": "qmd/15-multivariate-small-streams.html#nmds",
    "title": "15-Multivariate Analysis of Macrozoobenthos Samples from Two Small Streams",
    "section": "3.1 NMDS",
    "text": "3.1 NMDS\nWe start with an NMDS (nonmetric multidimensional scaling) of the bio-data using the Bray-Curtis dissimilarity measure. It is the default of metaMDS, but we specify it explicitly to make the selection of the dissimilarity measure clearly visible in the code. Automatic transformation is switched off. This can be changed, depending on the properties of the data, or enabled “manually” for example with wisconsin(sqrt(bio)).\nThe function metaMDS then runs the NMDS several times with different starting values to avoid local minima. For difficult data sets, it may be necessary to increase the metaparameters try and trymax, see helpfile for details.\nAfter that, we should have a look at the stress value and the stressplot.\n\nmds &lt;- metaMDS(bio, distance = \"bray\", autotransform = FALSE)\nmds\nstressplot(mds)\n\nWe can then plot the results of the NMDS-ordination.\n\nplot(mds, type = \"t\")\n\nIn order to show the influence of environmental variables, we can fit vectors or factors to the ordination. In addition to this, we can show the significance of the fitted vectors. For getting reliable p-values, I recommend to increase permu to 3999 or 9999.\n\n## fit environmental factors and perform a permutations-test\nefit &lt;- envfit(mds ~ Hochwasser + Bach + Habitat, env, permu = 999)\nefit\n\nNow, we can visualize the complete result. Grey dotted zero-lines are added to make interpretation easier.\n\nplot(mds, type = \"t\")\nplot(efit, add = TRUE)\nabline(h=0, col=\"grey\", lty=\"dotted\")\nabline(v=0, col=\"grey\", lty=\"dotted\")"
  },
  {
    "objectID": "qmd/15-multivariate-small-streams.html#hierarchical-clustering",
    "href": "qmd/15-multivariate-small-streams.html#hierarchical-clustering",
    "title": "15-Multivariate Analysis of Macrozoobenthos Samples from Two Small Streams",
    "section": "3.2 Hierarchical Clustering",
    "text": "3.2 Hierarchical Clustering\nThe NMDS tries to project the distances as good as possible to a low number of dimensions, e.g. k=2 that is the default. To see the full picture of distances in multidimensional space, we may consider to apply hierarchical clustering. As agglomeration algorithm, complete, ward.D2 or ward.D can be a good choice. To improve understanding it can be a good idea to compare it with other agglomeration schemes, e.g. single.\n\nhc &lt;- hclust(vegdist(bio), method=\"ward.D\")\nplot(hc)\n\nIt is also possible, to colorize the clusters in the NMDS plot. Let’s assume we have 4 clusters, we can first indicate it in the hierarchical cluster tree with rect.hclust``, then cut the tree withcutree`.\n\nplot(hc)\nrect.hclust(hc, 4)\ngrp &lt;- cutree(hc, 4)  # assign observations to 4 groups\ngrp\n\nThe result is an assignment of the original observations to groups, that can be used to colorize the NMDS plot. It is possible to show the cluster tree directly in the nmds plot or to indicate it otherwise with, for example, ordispider, ordihull or ordiellipse\n\nplot(mds, type = \"n\")\ntext(mds$points, row.names(bio), col = grp)\n\n## optional: show cluster tree\n#ordicluster(mds, hc, col=\"blue\")\n\nExercises: Compare different agglomeration schemes, try different numbers of clusters in rect.hclust and cutree and add the fitted environmental variables in the final plot."
  },
  {
    "objectID": "qmd/15-multivariate-small-streams.html#canonical-correspondence-analysis",
    "href": "qmd/15-multivariate-small-streams.html#canonical-correspondence-analysis",
    "title": "15-Multivariate Analysis of Macrozoobenthos Samples from Two Small Streams",
    "section": "3.3 Canonical Correspondence Analysis",
    "text": "3.3 Canonical Correspondence Analysis\nAs an alternative to NMDS, we can also use CCA, that is a “constraied ordination method” and allows a more detailed numerical analysis (e.g. separatation of inertia), but is limited to \\(\\chi^2\\)-distance, while NMDS allows arbitrary distance measures, including Bray-Curtis.\n\ncc &lt;- cca(bio ~ Habitat + Bach + Hochwasser, data = env)\n#cc &lt;- cca(bio ~ ., data = env) # same. The . means all from env\ncc # print Eigenvalues\nplot(cc)\nordihull(cc, env$Habitat, col = \"blue\")   # or: ordispider, ordiellipse ..."
  },
  {
    "objectID": "qmd/15-multivariate-small-streams.html#test-of-significance",
    "href": "qmd/15-multivariate-small-streams.html#test-of-significance",
    "title": "15-Multivariate Analysis of Macrozoobenthos Samples from Two Small Streams",
    "section": "3.4 Test of significance",
    "text": "3.4 Test of significance\nThe CCA supports also significance tests and model selection with ANOVA-like permutation tests.\n\n## Resampling-ANOVAs of the CCA\nanova(cc)\nanova(cc, by = \"terms\") # most useful\nanova(cc, by = \"axis\")\n\n## Model selection to find the optimal model\nstep(cc)\n\nSeveral other multivariate significance tests exist. The Adonis-Test is in particular popular, because it considers also interaction terms. It does not rely on an NMDS or CCA and works directly with a distance matrix. In order to increase its power, we may optionally consider strata. The following shows some examples.\nExercise: Try different model formulae and decide which one is most appropriate for the data set and the original hypothesis.\n\ndist &lt;- vegdist(bio, method = \"bray\")\n\nadonis2(dist ~ Hochwasser * Habitat * Bach, data = dat, by = \"terms\")\n\n## Comparison with and without strata\nadonis2(dist ~ Hochwasser * Bach, strata = env$Habitat, data = dat, by = \"terms\")\nadonis2(dist ~ Hochwasser * Bach, data = dat, by = \"terms\")"
  },
  {
    "objectID": "qmd/15-multivariate-small-streams.html#dbrda-and-elimination-of-covariates",
    "href": "qmd/15-multivariate-small-streams.html#dbrda-and-elimination-of-covariates",
    "title": "15-Multivariate Analysis of Macrozoobenthos Samples from Two Small Streams",
    "section": "3.5 dbRDA and elimination of covariates",
    "text": "3.5 dbRDA and elimination of covariates\nThe following examples show further possibilities. Instead of a CCA (that uses \\(\\chi^2\\)) we can also use a so-called distance-based redundancy analysis (dbRDA), that supports arbitrary distance measures, e.g. Bray-Curtis.\nAnother option is a partial CCA or partial dbRDA where we can eliminate covariates (condtion = ...) that we are not much interested in, so that the ordination focuses on the variables we are interested in. This is the called a partial analysis (pCCA, p-dbRDA). We will then also get three kinds of eigenvalues and eigenvectors (components of the inertia).\n\n## =========================================================================\n## dbRDA, supports arbitrary dissimilarity measures\n## =========================================================================\n\ndbr &lt;- dbrda(dist ~ Habitat + Bach + Hochwasser, data = env, distance = \"bray\")\ndbr\nanova(dbr, by=\"terms\", permutations=3999)\n#summary(dbr)\nplot(dbr)\n\n## =========================================================================\n## partial CCA: elimination of covariates\n## =========================================================================\npcc &lt;- cca(bio ~ Hochwasser + Bach + Condition(Habitat), data = env)\npcc\nplot(pcc)\n\n\n## =========================================================================\n## partial dbRDA\n## =========================================================================\ndbrc &lt;- dbrda(bio ~ Hochwasser + Bach + Condition(Habitat), \n              data = env, distance = \"bray\")\ndbrc\nplot(dbrc)\n\nExercise: Apply a method that eliminates the differences between the streams and investigate whether pools and riffles behave differently."
  },
  {
    "objectID": "qmd/15-multivariate-small-streams.html#procrustes-test",
    "href": "qmd/15-multivariate-small-streams.html#procrustes-test",
    "title": "15-Multivariate Analysis of Macrozoobenthos Samples from Two Small Streams",
    "section": "3.6 Procrustes test",
    "text": "3.6 Procrustes test\nTo compare the ordinations, that we get with a different set of environmental variable and conditions, we can use the so-called Procrustes test.\n\ndbr  &lt;- dbrda(bio ~ Hochwasser + Bach + Habitat, \n              data = env, distance=\"bray\")\npdbr &lt;- dbrda(bio ~ Hochwasser + Bach + Condition(Habitat), \n              data = env, distance=\"bray\")\n\nproc &lt;- procrustes(dbr, pdbr)\nplot(proc, type = \"t\")\nprotest(dbr, pdbr)"
  },
  {
    "objectID": "qmd/13-timeseries-breakpoints.html",
    "href": "qmd/13-timeseries-breakpoints.html",
    "title": "13-Identification of Breakpoints in Time Series",
    "section": "",
    "text": "The example is adapted from the help pages of R package strucchange, (Zeileis et al., 2002). The scientific question is to detect breakpoints where the hydrological regime of a river suddenly changed due to management changes, e.g. dam construction."
  },
  {
    "objectID": "qmd/13-timeseries-breakpoints.html#dataset",
    "href": "qmd/13-timeseries-breakpoints.html#dataset",
    "title": "13-Identification of Breakpoints in Time Series",
    "section": "2.1 Dataset",
    "text": "2.1 Dataset\nThe dataset is a hydrological time series of the discharge of the river Nile measured at Aswan in 108 m3 a-1. The origin of the data is described in Cobb (1978). It is contained in the strucchange package and can be loaded with data(Nile).\n\nlibrary(\"strucchange\")\ndata(\"Nile\")\nplot(Nile)"
  },
  {
    "objectID": "qmd/13-timeseries-breakpoints.html#data-analysis",
    "href": "qmd/13-timeseries-breakpoints.html#data-analysis",
    "title": "13-Identification of Breakpoints in Time Series",
    "section": "2.2 Data analysis",
    "text": "2.2 Data analysis\nThe data analysis is carried out in several steps:\n\nStatistical test if the time series contains breakpoints\nIdentification of number and position of breakpoints\nVisualization of model results\nDiagnostic tests"
  },
  {
    "objectID": "qmd/13-timeseries-breakpoints.html#test-of-existence-of-breakpoints",
    "href": "qmd/13-timeseries-breakpoints.html#test-of-existence-of-breakpoints",
    "title": "13-Identification of Breakpoints in Time Series",
    "section": "3.1 Test of existence of breakpoints",
    "text": "3.1 Test of existence of breakpoints\nHere we use a so-called OLS-CUSUM test (ordinary least-squares regression, cumulative sums). The technical procedure is that we first define a null hypothesis for an empirical fluctuation process (efp). Here the model Nile ~ 1 means that we assume a constant mean value over time without trend. In case we allow for a linear trend, we could use Nile ~ time (Nile).\nThe ocus (OLS-CUSUM) object is then plotted and a structural change test (sctest) applied. The y-axis of the plot is scaled in units of standard deviations. The line shows the cumulative sum of deviations from the mean value. An monotonous increase of the line means that values are above the arithmetic mean, a decrease that they are below average. If the line exceeds the horizontal confidence bands, it indicates a structural change.\n\nocus &lt;- efp(Nile ~ 1, type = \"OLS-CUSUM\")\nplot(ocus)\n\n\n\n\n\n\n\nsctest(ocus)\n\n\n    OLS-based CUSUM test\n\ndata:  ocus\nS0 = 2.9518, p-value = 5.409e-08"
  },
  {
    "objectID": "qmd/13-timeseries-breakpoints.html#identification-of-structural-breaks",
    "href": "qmd/13-timeseries-breakpoints.html#identification-of-structural-breaks",
    "title": "13-Identification of Breakpoints in Time Series",
    "section": "3.2 Identification of structural breaks",
    "text": "3.2 Identification of structural breaks\nFunction breakpointsis the main workhorse of the package. It iteratively scans the time series for candidate breakpoints, that can be printed. The BIC values returned by summary or and that are visible in the plot are then used for model selection. We select the model with the minimum BIC. Here we use again a model without trend (Nile ~ 1).\n\nbp.nile &lt;- breakpoints(Nile ~ 1)\nsummary(bp.nile)\n\nplot(bp.nile)\n\nTask: find out how many breakpoints are necessary for an optimal model and at which time they occured."
  },
  {
    "objectID": "qmd/13-timeseries-breakpoints.html#visualisation",
    "href": "qmd/13-timeseries-breakpoints.html#visualisation",
    "title": "13-Identification of Breakpoints in Time Series",
    "section": "3.3 Visualisation",
    "text": "3.3 Visualisation\nIn the following, we compare the model with N breakpoints with the null model fm0 without any breakpoint. In addition, we can also show indcate the confidence interval.\nTask: replace xxxwith the correct number of breakpoints. It is also possible, to try other numbers of breakpoints to understand the algorith. Finally set it back to the optimal value.\n\nfm0 &lt;- lm(Nile ~ 1)\nfm1 &lt;- lm(Nile ~ breakfactor(bp.nile,  breaks = xxx))\nplot(Nile)\nlines(ts(fitted(fm0),  start = 1871),  col = 3)\nlines(ts(fitted(fm1),  start = 1871),  col = 4)\nlines(bp.nile)\n\n## confidence interval\nci.nile &lt;- confint(bp.nile)\nci.nile\nlines(ci.nile)\n\nThe optional code uses a simpler and less fancy method for indicating the breakpoint(s).\n\nplot(Nile)\ndat &lt;- data.frame(time = time(Nile), Q = as.vector(Nile))\nabline(v = dat$time[bp.nile$breakpoints],  col = \"green\")\n\nIf we need a p-value, we can compare the two models with a likelihood ratio test:\n\n## ANOVA test whether the two models are significantly different\nanova(fm0, fm1)"
  },
  {
    "objectID": "qmd/13-timeseries-breakpoints.html#diagnostics",
    "href": "qmd/13-timeseries-breakpoints.html#diagnostics",
    "title": "13-Identification of Breakpoints in Time Series",
    "section": "3.4 Diagnostics",
    "text": "3.4 Diagnostics\nFinally let’s check autocorrelation and normality of residuals. In case the breakpoint model was appropriate, autocorrelations should vanish. As a counterexample, we plot also the autocorrelation function of the null model (fm1).\nFinally, we check residuals for approximate normality.\n\npar(mfrow=c(2, 2))\nacf(residuals(fm0))\nacf(residuals(fm1))\nqqnorm(residuals(fm0))\nqqnorm(residuals(fm1))"
  },
  {
    "objectID": "qmd/10-nonlinear-regression.html",
    "href": "qmd/10-nonlinear-regression.html",
    "title": "10-Nonlinear Regression",
    "section": "",
    "text": "The following examples demonstrate how to perform non-linear regression in R. This is quite different from linear regression, not only because the regression functions show a curve, but also due to the applied numerical techniques. While in the linear case, the coefficients of the regression line can be calculated directly (analytically) by solving a linear system of equations, iterative numerical optimization needs to be used instead.\nThis means that the coefficients are approximated step by step until convergence, beginning with start values specified by the user. There is no guarantee that an optimal solution can be found.\nThe following examples are intended as a starting point, the last example (logistic growth) is left as an exercise.\n\n\nThe first example shows an exponentially growing data set that is fitted by nonlinear least squares (nls). Here, the exponential function is given directly at the right hand side of the formula and the start values for the parameters are specified in pstart. An optional argument trace = TRUE is set to watch the progress of iteration.\nFunction predict can be used to create dense \\(x\\) and \\(y\\)-values (here: x1 and y1) for a smooth curve.\nThe statistical results of the regression are displayed with summary: parameters (\\(a\\), \\(b\\)) of the curve, standard errors and the correlation between \\(a\\) and \\(b\\). Note the optional correlation=TRUE argument. High correlations can indicate problems in model fitting, especially due to so called non-identifiablility of the parameter set.\nHere everything went through even with high correlation, because the data do not vary much around the exponential curve.\nFinally, the nonlinear coefficient of determination \\(R^2\\) can be calculated from the variances of the residuals and the original data:\n\nx &lt;- 1:10\ny &lt;- c(1.6, 1.8, 2.1, 2.8, 3.5, 4.1, 5.1, 5.8, 7.1, 9.0)\nplot(x, y)\n\npstart &lt;- c(a = 1, b = 1)\nmodel_fit &lt;- nls(y ~ a * exp(b * x), start = pstart, trace = TRUE)\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(model_fit, data.frame(x = x1))\nlines(x1, y1, col = \"red\")\n\nsummary(model_fit, correlation = TRUE)\n\n## R squared\n1 - var(residuals(model_fit))/var(y)\n\n\n\n\nInstead of putting the regression model directly into nls it is also possible to use a user-defined function, that we call f here for example. This approach is especially useful for more complicated regression models:\n\nf &lt;- function(x, a, b) {a * exp(b * x)}\n\nplot(x, y)\n\npstart &lt;- c(a = 1, b = 1)\nmodel_fit &lt;- nls(y ~ f(x, a, b), start = pstart)\n\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(model_fit, data.frame(x = x1))\nlines(x1, y1, col = \"red\")\nsummary(model_fit, correlation = TRUE)\n1 - var(residuals(model_fit))/var(y)"
  },
  {
    "objectID": "qmd/10-nonlinear-regression.html#exponential-growth",
    "href": "qmd/10-nonlinear-regression.html#exponential-growth",
    "title": "10-Nonlinear Regression",
    "section": "",
    "text": "The first example shows an exponentially growing data set that is fitted by nonlinear least squares (nls). Here, the exponential function is given directly at the right hand side of the formula and the start values for the parameters are specified in pstart. An optional argument trace = TRUE is set to watch the progress of iteration.\nFunction predict can be used to create dense \\(x\\) and \\(y\\)-values (here: x1 and y1) for a smooth curve.\nThe statistical results of the regression are displayed with summary: parameters (\\(a\\), \\(b\\)) of the curve, standard errors and the correlation between \\(a\\) and \\(b\\). Note the optional correlation=TRUE argument. High correlations can indicate problems in model fitting, especially due to so called non-identifiablility of the parameter set.\nHere everything went through even with high correlation, because the data do not vary much around the exponential curve.\nFinally, the nonlinear coefficient of determination \\(R^2\\) can be calculated from the variances of the residuals and the original data:\n\nx &lt;- 1:10\ny &lt;- c(1.6, 1.8, 2.1, 2.8, 3.5, 4.1, 5.1, 5.8, 7.1, 9.0)\nplot(x, y)\n\npstart &lt;- c(a = 1, b = 1)\nmodel_fit &lt;- nls(y ~ a * exp(b * x), start = pstart, trace = TRUE)\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(model_fit, data.frame(x = x1))\nlines(x1, y1, col = \"red\")\n\nsummary(model_fit, correlation = TRUE)\n\n## R squared\n1 - var(residuals(model_fit))/var(y)"
  },
  {
    "objectID": "qmd/10-nonlinear-regression.html#regression-model-as-user-defined-function",
    "href": "qmd/10-nonlinear-regression.html#regression-model-as-user-defined-function",
    "title": "10-Nonlinear Regression",
    "section": "",
    "text": "Instead of putting the regression model directly into nls it is also possible to use a user-defined function, that we call f here for example. This approach is especially useful for more complicated regression models:\n\nf &lt;- function(x, a, b) {a * exp(b * x)}\n\nplot(x, y)\n\npstart &lt;- c(a = 1, b = 1)\nmodel_fit &lt;- nls(y ~ f(x, a, b), start = pstart)\n\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(model_fit, data.frame(x = x1))\nlines(x1, y1, col = \"red\")\nsummary(model_fit, correlation = TRUE)\n1 - var(residuals(model_fit))/var(y)"
  },
  {
    "objectID": "qmd/09-clementines-anova.html",
    "href": "qmd/09-clementines-anova.html",
    "title": "09-Which fruits are the biggest? An ANOVA example",
    "section": "",
    "text": "Preface\nThe following template is intended as a starting point for a short report of an ANOVA. It is recommended to follow the general outline (Introduction, Methods, Results, Discussion), but to adapt the content to the specific needs of the analysis.\nIt is good practice to concentrate on the main findings and their discussion and to avoid unnecessary technical detail. The report should be illustrated with meaningful (and only the most important) tables and graphs.\nThe data set consists of different samples of clementine fruits from different brands and different shops. The data sets can be downloaded from\nhttps://tpetzoldt.github.io/datasets/.\nA description is found in the file clementines_info.txt."
  },
  {
    "objectID": "qmd/09-clementines-anova.html#data",
    "href": "qmd/09-clementines-anova.html#data",
    "title": "09-Which fruits are the biggest? An ANOVA example",
    "section": "2.1 Data",
    "text": "2.1 Data\n\nwrite something about the data\n\ndescribe the different samples and how they were obtained\nweight determination with a scale, length determination with a caliper (see below)\n\nread the data in:\n\n\nbrands &lt;- read.csv(\"clementines2022-brands.csv\")\nfruits &lt;- read.csv(\"clementines2022-fruits.csv\")\n\nShow the structure of the data. Use the data explorer of RStudio or the head-function, that shows the first lines of each data set. Please do not forget to report the sample size of your data!\n\ncat(\"sample size:\", nrow(fruits), \"\\n\")\nhead(fruits, n=3)\n\nLook also at the structure of brands.\nThen join the two tables and convert the brand column into a factor variable.\n\ndat &lt;- left_join(fruits, brands, by=\"brand\")\ndat$brand &lt;- factor(dat$brand)"
  },
  {
    "objectID": "qmd/09-clementines-anova.html#data-analysis",
    "href": "qmd/09-clementines-anova.html#data-analysis",
    "title": "09-Which fruits are the biggest? An ANOVA example",
    "section": "2.2 Data analysis",
    "text": "2.2 Data analysis\nMention R (R Core Team, 2024) in a single sentence, cite special packages. Write that an ANOVA was performed and which methods were used in addition."
  },
  {
    "objectID": "qmd/09-clementines-anova.html#hypotheses",
    "href": "qmd/09-clementines-anova.html#hypotheses",
    "title": "09-Which fruits are the biggest? An ANOVA example",
    "section": "5.1 Hypotheses",
    "text": "5.1 Hypotheses\nIt is important to formulate clear hypotheses. Here are a few examples, related to the data set from 2019. Please think about it and define your own hypotheses, related to the current data set.\n\n\\(H_{0,1}\\): The mean weight of the fruits is the same in all samples.\n\\(H_{A,1}\\): The weight is different in any of the samples.\n\\(H_{A,2}\\): Which brand is the smallest? The mean weight of smallest sample is significantly smaller than of the 2nd samplest sample.\n\\(H_{A,3}\\): The fruits from the premium brands are bigger than corresponding basic brands.\n\nNote: Hypotheses \\(H_{A,2}\\) and \\(H_{A,3}\\) have their own, different \\(H_0\\).\nA hypothesis like \\(H_{A,3}\\) is more difficult and optional. It requires a two-way ANOVA and can only be applied to a subset of the data, where different brands from the same shops are available."
  },
  {
    "objectID": "qmd/09-clementines-anova.html#measurement-methods",
    "href": "qmd/09-clementines-anova.html#measurement-methods",
    "title": "09-Which fruits are the biggest? An ANOVA example",
    "section": "5.2 Measurement methods",
    "text": "5.2 Measurement methods\n\nWeight: was determined with a kitchen scale in gramm (g).\nHeight and Width: were measured with a caliper (Fig. 1)\n\n\n\n\n\nSize measurement with a digital caliper."
  },
  {
    "objectID": "qmd/09-clementines-anova.html#r-example-code",
    "href": "qmd/09-clementines-anova.html#r-example-code",
    "title": "09-Which fruits are the biggest? An ANOVA example",
    "section": "5.3 R example code",
    "text": "5.3 R example code\n\nThe following code is intended as a starting example. It is recommended to adapt the script to analyse the data from another year.\nDon’t forget that it is an exercise, not a serious analysis, so feel free to create your own story.\nDon’t make your report too technical, concentrate on your message.\n\n\nbrands &lt;- read.csv(\"https://tpetzoldt.github.io/datasets/data/clementines2019-brands.csv\")\nfruits &lt;- read.csv(\"https://tpetzoldt.github.io/datasets/data/clementines2019-fruits.csv\")\n\ndat &lt;- left_join(fruits, brands) # merge tables by their common colum 'Brand'\ndat$brand &lt;- factor(dat$brand)\n\nboxplot(weight ~ brand, data=dat)\n\n## the ANOVA\nm &lt;- lm(weight ~ brand, data=dat)\nanova(m)\n\n## posthoc test\nTukeyHSD(aov(m))\n\n## graphical display of Tukey's test\nplot(TukeyHSD(aov(m)), las=1, cex.axis=0.5)\n\n## graphical and numerical checks of variance homogeneity\nplot(m, which=1)\nfligner.test(weight ~ brand, data=dat)\n\n## approximate normality of residuals\nplot(m, which=2)\n\n## optional: special one-way anova alternative if variances are unequal\noneway.test(weight ~ brand, data=dat)"
  },
  {
    "objectID": "qmd/09-clementines-anova.html#text-processing",
    "href": "qmd/09-clementines-anova.html#text-processing",
    "title": "09-Which fruits are the biggest? An ANOVA example",
    "section": "5.4 Text processing",
    "text": "5.4 Text processing\nThe report can, in principle, be written in any text processing software, e.g. Microsoft Word or LibreOffice Write, but I recommend to try Quarto. It needs only a little extra learning, but is an extremely efficient way to combine text and analysis with R in one document and write it directly in RStudio. A comprehensive documentation can be found on https://quarto.org/."
  },
  {
    "objectID": "qmd/09-clementines-anova.html#questions-and-literature-research",
    "href": "qmd/09-clementines-anova.html#questions-and-literature-research",
    "title": "09-Which fruits are the biggest? An ANOVA example",
    "section": "5.5 Questions and literature research",
    "text": "5.5 Questions and literature research\nPlease post questions, comments and ideas to the chat group. As the exercise is a toy example, it may be difficult to find relevant citeable literature. However, we don’t limit creativity.\nReferences"
  },
  {
    "objectID": "qmd/07-correlation.html",
    "href": "qmd/07-correlation.html",
    "title": "07-Correlation Between Water Quality Indicators",
    "section": "",
    "text": "Given are the following series of measurements from a polluted river (Ertel et al., 2012):\n\nStation &lt;- 1:17\nCOD   &lt;- c(9.6, 12.2, 13.2, 13.3, 37.4, 21.4, 16.1, 24.8, 24.2, 26.9,\n           29.2, 31.6, 18.2, 24.8, 13.7, 23.6, 24.2)\nO2    &lt;- c(9.8, 10, 8.3, 9.6, 1.5, 4.4, 6.3, 3, 3, 10, 9.4, 17.9, 9.7,\n           9.7, 8.2, 9.5, 11.3)\nNH4   &lt;- c(0.15, 0.1, 0.74, 0.29, 5.04, 2.26, 0.96, 3.37, 2.44, 0.27,\n          0.32, 0.68, 0.27, 0.32, 0.22, 0.58, 0.59)\nColor &lt;- c(0.59, 0.52, 0.6, 0.57, 1.34, 1.21, 1.17, 1.12, 1.1, 1.08,\n           1.24, 1.25, 1.29, 1.29, 1.06, 1.25, 1.16)\n\nwith COD, the chemical oxygen demand (in \\(\\mathrm{mg L^{-1}}\\)), \\(\\mathrm O_2\\) oxygen concentration (\\(\\mathrm{mg L^{-1}}\\)) and Color. The spectral absorption coefficient at 436nm (\\(\\mathrm m^{-1}\\)) is a measure of the color intensity of the filtered water. The data set is a subset from field measurements, that contained more samples from several measurement campaigns.\nThe question is whether Ammonium (NH4), Oxygen (O2) and Color depend on the organic load (COD).\n\n\n\nFor the dependency between Color and COD we can use Pearson correlation. In addition it is always a good idea to plot the data.\n\nplot(COD, Color)\ncor.test(COD, Color)\n\nWe see an almost linear dependency and get a highly significant correlation. Now, for the dependence between Ammonium on COD we can proceed with:\n\nplot(COD, NH4)\ncor.test(COD, NH4)\n\nWe find again significant correlation. However, the dependency is not very strict and it seems that there are two different data sets. This is in fact true because sampling sites 1–9 were before and the other sampling sites below a cooling reservoir of a power plant. In the following, we create first an empty plot(...., type = \"n\") and then add the station number as text labels:\n\nplot(COD, NH4, type = \"n\")\ntext(COD, NH4, labels = Station)\n\nWe now repeat the analysis with the first 9 data pairs from the stations upstream of the cooling reservoir (50.19N, 24.40E, Link to Google Maps):\n\nplot(COD[1:9], NH4[1:9])\ncor.test(COD[1:9], NH4[1:9])\n\n\n\n\nIs oxygen concentration (O2) directly related to organic pollution (COD)? Interpret the results, and discuss the potential mechanisms how the cooling reservoir may influence water quality. Compare your conclusions with the paper of Ertel et al. (2012). Does the assumption of independent residuals hold?\nRepeat the analysis with Spearman’s correlation, and compare the results with the Pearson correlation coefficients."
  },
  {
    "objectID": "qmd/07-correlation.html#introduction",
    "href": "qmd/07-correlation.html#introduction",
    "title": "07-Correlation Between Water Quality Indicators",
    "section": "",
    "text": "Given are the following series of measurements from a polluted river (Ertel et al., 2012):\n\nStation &lt;- 1:17\nCOD   &lt;- c(9.6, 12.2, 13.2, 13.3, 37.4, 21.4, 16.1, 24.8, 24.2, 26.9,\n           29.2, 31.6, 18.2, 24.8, 13.7, 23.6, 24.2)\nO2    &lt;- c(9.8, 10, 8.3, 9.6, 1.5, 4.4, 6.3, 3, 3, 10, 9.4, 17.9, 9.7,\n           9.7, 8.2, 9.5, 11.3)\nNH4   &lt;- c(0.15, 0.1, 0.74, 0.29, 5.04, 2.26, 0.96, 3.37, 2.44, 0.27,\n          0.32, 0.68, 0.27, 0.32, 0.22, 0.58, 0.59)\nColor &lt;- c(0.59, 0.52, 0.6, 0.57, 1.34, 1.21, 1.17, 1.12, 1.1, 1.08,\n           1.24, 1.25, 1.29, 1.29, 1.06, 1.25, 1.16)\n\nwith COD, the chemical oxygen demand (in \\(\\mathrm{mg L^{-1}}\\)), \\(\\mathrm O_2\\) oxygen concentration (\\(\\mathrm{mg L^{-1}}\\)) and Color. The spectral absorption coefficient at 436nm (\\(\\mathrm m^{-1}\\)) is a measure of the color intensity of the filtered water. The data set is a subset from field measurements, that contained more samples from several measurement campaigns.\nThe question is whether Ammonium (NH4), Oxygen (O2) and Color depend on the organic load (COD)."
  },
  {
    "objectID": "qmd/07-correlation.html#data-analysis",
    "href": "qmd/07-correlation.html#data-analysis",
    "title": "07-Correlation Between Water Quality Indicators",
    "section": "",
    "text": "For the dependency between Color and COD we can use Pearson correlation. In addition it is always a good idea to plot the data.\n\nplot(COD, Color)\ncor.test(COD, Color)\n\nWe see an almost linear dependency and get a highly significant correlation. Now, for the dependence between Ammonium on COD we can proceed with:\n\nplot(COD, NH4)\ncor.test(COD, NH4)\n\nWe find again significant correlation. However, the dependency is not very strict and it seems that there are two different data sets. This is in fact true because sampling sites 1–9 were before and the other sampling sites below a cooling reservoir of a power plant. In the following, we create first an empty plot(...., type = \"n\") and then add the station number as text labels:\n\nplot(COD, NH4, type = \"n\")\ntext(COD, NH4, labels = Station)\n\nWe now repeat the analysis with the first 9 data pairs from the stations upstream of the cooling reservoir (50.19N, 24.40E, Link to Google Maps):\n\nplot(COD[1:9], NH4[1:9])\ncor.test(COD[1:9], NH4[1:9])"
  },
  {
    "objectID": "qmd/07-correlation.html#exercises-and-discussion",
    "href": "qmd/07-correlation.html#exercises-and-discussion",
    "title": "07-Correlation Between Water Quality Indicators",
    "section": "",
    "text": "Is oxygen concentration (O2) directly related to organic pollution (COD)? Interpret the results, and discuss the potential mechanisms how the cooling reservoir may influence water quality. Compare your conclusions with the paper of Ertel et al. (2012). Does the assumption of independent residuals hold?\nRepeat the analysis with Spearman’s correlation, and compare the results with the Pearson correlation coefficients."
  },
  {
    "objectID": "qmd/07-correlation.html#introduction-1",
    "href": "qmd/07-correlation.html#introduction-1",
    "title": "07-Correlation Between Water Quality Indicators",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nGiven is a number of students that passed an examination in statistics. The examination was written two times, one time before and one time after an additional series of lectures. The values represent the number of points approached during the examination. Check whether there is a dependency between the results before and after the test, i.e. if there is any dependency between the results of the final test and the basic knowledge before the course."
  },
  {
    "objectID": "qmd/07-correlation.html#data-and-data-analysis",
    "href": "qmd/07-correlation.html#data-and-data-analysis",
    "title": "07-Correlation Between Water Quality Indicators",
    "section": "2.2 Data and data analysis",
    "text": "2.2 Data and data analysis\n\nx1 &lt;- c(69, 77, 35, 34, 87, 45, 95, 83)\nx2 &lt;- c(100, 97, 67, 42, 75, 73, 92, 97)\ncor.test(x1, x2)\n\nIf we plot this, we see a rather strange pattern, i.e. no clear linear relationship:\n\nplot(x1, x2)\n\nTherefore it may be a good idea to use the rank correlation:\n\ncor.test(x1, x2, method=\"spearman\")\n\nSometimes, we may get a warning that it “cannot compute exact p-values with ties”, then we can use another approach and compute the Spearman correlation via the Pearson correlation of ranks:\n\ncor.test(rank(x1), rank(x2))\n\nThis needs a little bit more effort (for the computer, not for us), but the interpretation is the same. To understand how this worked, it can be a good idea to create a scatterplot of the ranks of both variables."
  },
  {
    "objectID": "qmd/07-correlation.html#exercise-and-discussion",
    "href": "qmd/07-correlation.html#exercise-and-discussion",
    "title": "07-Correlation Between Water Quality Indicators",
    "section": "2.3 Exercise and Discussion",
    "text": "2.3 Exercise and Discussion\nWhat do the results above tell us? Compare the results with a paired t-test of the same data set. Which test tells what?"
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html",
    "href": "qmd/05-distributions-fruits-tidyverse.html",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "",
    "text": "The example aims to demonstrate estimation and interpretation of confidence intervals. At the end, the two samples are compared with respect to variance and mean values.\nThe experimental hypotheses was, that weight and size of two samples of Clementine fruits differ. The result is to be visualized with bar charts or box plots. We use only the weight as an example, analysis of the other statistical parameters is left as an optional exercise.\nWe can now derive the following statistical hypotheses about the variance:\n\n\\(H_0\\): The variance of both samples is the same.\n\\(H_a\\): The samples have different variance.\n\nand about the mean:\n\n\\(H_0\\): The mean of both samples is the same.\n\\(H_a\\): The mean values of the samples are different."
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#prepare-and-inspect-data",
    "href": "qmd/05-distributions-fruits-tidyverse.html#prepare-and-inspect-data",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "3.1 Prepare and inspect data",
    "text": "3.1 Prepare and inspect data\n\nDownload the data set fruits-2023-hse.csv and use one of RStudio’s “Import Dataset” wizards.\nA better alternative is to use read.csv().\nThe data set is available in OPAL1 or from: https://raw.githubusercontent.com/tpetzoldt/datasets/refs/heads/main/data/fruits-2023-hse.csv\n\n\n#  ... do it\n\n\nplot everything, just for testing:\n\n\nplot(fruits)\n\n\nsplit table for box1 and box2:\n\n\nbox1 &lt;- subset(fruits, brand == \"box1\")\nbox2 &lt;- subset(fruits, brand == \"box2\")\n\n\ncompare weight of both groups:\n\n\nboxplot(box1$weight, box2$weight, names=c(\"box1\", \"box2\"))\n\nNote: It is also possible to use boxplot with the model formula syntax. This is the preferred way, because it does not require to split the data set beforehand:\n\nboxplot(weight ~ brand, data = fruits)"
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#check-distribution",
    "href": "qmd/05-distributions-fruits-tidyverse.html#check-distribution",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "3.2 Check distribution",
    "text": "3.2 Check distribution\nWe can check the shape of distribution graphically. If mean values of the samples differ much, it has to be done separately for each sample.\n\n# use `hist`, `qqnorm`, `qqline`\n# ..."
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#sample-statistics",
    "href": "qmd/05-distributions-fruits-tidyverse.html#sample-statistics",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "3.3 Sample statistics",
    "text": "3.3 Sample statistics\nIf we assume normal distribution of the data, we can estimate an approximate prediction interval from the sample parameters, i.e. in which size range we find 95% of the weights within one group.\nWe first calculate mean, sd, N and se for “box1” data set:\n\nbox1.mean &lt;- mean(box1$weight)\nbox1.sd   &lt;- sd(box1$weight)\nbox1.N    &lt;- length(box1$weight)\nbox1.se   &lt;- box1.sd/sqrt(box1.N)\n\nThen we estimate the two-sided 95% prediction interval for the sample, assuming normal distribution:\n\nbox1.95 &lt;- box1.mean + c(-1.96, 1.96) * box1.sd\nbox1.95\n\nInstead of using 1.96, we could also use the quantile function of the normal distribution instead, e.g. qnorm(0.975)for the upper interval or qnorm(c(0.025, 0.975)) for the lower and upper.\nIf the data set is large enough, we can compare the prediction interval from above with the empirical quantiles, i.e. take it directly from the data. Here we do not assume a normal or any other distribution.\n\nquantile(box1$weight, p = c(0.025, 0.975))\n\nNow we plot the data and indicate the 95% interval:\n\nplot(box1$weight)\nabline(h = box1.95, col=\"red\")\n\n… and the same as histogram:\n\nhist(box1$weight)\nabline(v = box1.95, col=\"red\")\nrug(box1$weight, col=\"blue\")"
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#confidence-interval-of-the-mean",
    "href": "qmd/05-distributions-fruits-tidyverse.html#confidence-interval-of-the-mean",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "3.4 Confidence interval of the mean",
    "text": "3.4 Confidence interval of the mean\nThe confidence interval of the mean tells us how precise a mean value was estimated from data. If the sample size is “large enough”, the distribution of the raw data does not necessarily need to be normal distributed, because then mean values tend to approximate a normal distribution due to the central limit theorem.\n\n3.4.1 Confidence interval of the mean for the “box1” data\n\nCalculate the confidence interval of the mean value of the “box1” data set,\nuse +/- 1.96 or (better) the quantile of the t-distribution:\n\n\nbox1.ci &lt;- box1.mean + qt(p = c(0.025, 0.975), df = box1.N-1) * box1.se\n\nNow indicate the confidence interval of the mean in the histogram.\n\nabline(v = box1.ci, col=\"red\")\n\n\n\n3.4.2 Confidence interval for the mean of the “box2” data\nWe could now in principle do the same as above for the “box2” sample, but this would be rather cumbersome and boring. A more efficient method from package dplyr is shown below."
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#calculation-of-summary-statistics-with-dplyr",
    "href": "qmd/05-distributions-fruits-tidyverse.html#calculation-of-summary-statistics-with-dplyr",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "5.1 Calculation of summary statistics with dplyr",
    "text": "5.1 Calculation of summary statistics with dplyr\nSummarizing can be done with two functions, group_by that adds grouping information to a data frame and summarize to calculate summary statistics. In the following, we use the full data set with 4 groups.\n\nlibrary(\"dplyr\")\nfruits &lt;- read.csv(\"fruits-2023-hse.csv\")\n\nstats &lt;-\n  fruits |&gt;\n    group_by(brand) |&gt;\n    summarize(mean = mean(weight), sd=sd(weight), N=length(weight), se=sd/sqrt(N),\n              lwr = mean + qt(p = 0.025, df = N-1) * se,\n              upr = mean + qt(p = 0.975, df = N-1) * se\n             )\n\nstats"
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#barchart-and-errorbars-with-ggplot2",
    "href": "qmd/05-distributions-fruits-tidyverse.html#barchart-and-errorbars-with-ggplot2",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "5.2 Barchart and errorbars with ggplot2",
    "text": "5.2 Barchart and errorbars with ggplot2\nWe can then use the table of summary statistics directly for a bar chart.\n\nlibrary(\"ggplot2\")\nstats |&gt;\n  ggplot(aes(x=brand, y=mean, min=lwr, max=upr))  +\n    geom_col() + geom_errorbar()"
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#additional-tasks",
    "href": "qmd/05-distributions-fruits-tidyverse.html#additional-tasks",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "5.3 Additional tasks",
    "text": "5.3 Additional tasks\nRepeat the analysis with other properties of the fruits, e.g. width and height. Create box plots, analyse distribution, create bar charts."
  },
  {
    "objectID": "qmd/05-distributions-fruits-tidyverse.html#footnotes",
    "href": "qmd/05-distributions-fruits-tidyverse.html#footnotes",
    "title": "05-Distribution and Confidence Intervals of Clementines",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOPAL is the learning management system used at TU Dresden.↩︎"
  },
  {
    "objectID": "qmd/03-discharge-elbe.html",
    "href": "qmd/03-discharge-elbe.html",
    "title": "03-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R",
    "section": "",
    "text": "This practical example demonstrates how daily discharge data of the Elbe River can be analyzed and visualized in R directly in a browser using Web-R.\nScientific aim: Learn date/time computation, data management, aggregation, and plotting.",
    "crumbs": [
      "Labs",
      "03-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R"
    ]
  },
  {
    "objectID": "qmd/03-discharge-elbe.html#introduction",
    "href": "qmd/03-discharge-elbe.html#introduction",
    "title": "03-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R",
    "section": "",
    "text": "This practical example demonstrates how daily discharge data of the Elbe River can be analyzed and visualized in R directly in a browser using Web-R.\nScientific aim: Learn date/time computation, data management, aggregation, and plotting.",
    "crumbs": [
      "Labs",
      "03-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R"
    ]
  },
  {
    "objectID": "qmd/03-discharge-elbe.html#methods",
    "href": "qmd/03-discharge-elbe.html#methods",
    "title": "03-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R",
    "section": "2 Methods",
    "text": "2 Methods\nWe demonstrate data import, conversion, aggregation, plotting, and pipeline usage using the tidyverse. Data are daily measurements for the Elbe River (m³/s) from Dresden, provided by BfG.",
    "crumbs": [
      "Labs",
      "03-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R"
    ]
  },
  {
    "objectID": "qmd/03-discharge-elbe.html#exercises",
    "href": "qmd/03-discharge-elbe.html#exercises",
    "title": "03-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R",
    "section": "3 Exercises",
    "text": "3 Exercises\n\n3.1 1. Download the data and inspect it\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.2 2. Create date categories\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.3 3. Aggregate monthly data\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.4 4. Average year plot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.5 5. Annual discharge sum\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.6 6. Scatter plot per year\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.7 7. Cumulative sum per year\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.8 8. Min-Max Plot with ggplot2\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.9 9. Pivot table (wide ↔︎ long)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Labs",
      "03-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R"
    ]
  },
  {
    "objectID": "qmd/01-pivot-tables-with-libreoffice.html",
    "href": "qmd/01-pivot-tables-with-libreoffice.html",
    "title": "01-Discharge of River Elbe: Data Management with LibreOffice",
    "section": "",
    "text": "flowchart TD\n    A[Christmas] --&gt;|Get money| B(Go shopping)\n    B --&gt; C{Let me think}\n    C --&gt;|One| D[Laptop]\n    C --&gt;|Two| E[iPhone]\n    C --&gt;|Three| F[fa:fa-car Car]\n\n\n\n\n\n\nPlanning and maintenance of waterways and rivers needs adequate measurements and data. Raw time series can often be long and confusing, so a first step is aggregation and visualization.\nScientific aim: plot an average year and calculate monthly averages, minima and maxima of the discharge."
  },
  {
    "objectID": "qmd/01-pivot-tables-with-libreoffice.html#introduction",
    "href": "qmd/01-pivot-tables-with-libreoffice.html#introduction",
    "title": "01-Discharge of River Elbe: Data Management with LibreOffice",
    "section": "",
    "text": "flowchart TD\n    A[Christmas] --&gt;|Get money| B(Go shopping)\n    B --&gt; C{Let me think}\n    C --&gt;|One| D[Laptop]\n    C --&gt;|Two| E[iPhone]\n    C --&gt;|Three| F[fa:fa-car Car]\n\n\n\n\n\n\nPlanning and maintenance of waterways and rivers needs adequate measurements and data. Raw time series can often be long and confusing, so a first step is aggregation and visualization.\nScientific aim: plot an average year and calculate monthly averages, minima and maxima of the discharge."
  },
  {
    "objectID": "qmd/01-pivot-tables-with-libreoffice.html#methods",
    "href": "qmd/01-pivot-tables-with-libreoffice.html#methods",
    "title": "01-Discharge of River Elbe: Data Management with LibreOffice",
    "section": "2 Methods",
    "text": "2 Methods\nThe approach demonstrates the use of pivot tables for aggregating data according to certain criteria. We will also use date and time computation to derive aggregation criteria from a single date column.\nThe data set consists of daily measurements for discharge of the Elbe River in Dresden (daily discharge sum in \\(\\mathrm{m^3 d^{-1}}\\)).\nData were kindly provided by the German Federal Institute for Hydrology (BfG)."
  },
  {
    "objectID": "qmd/01-pivot-tables-with-libreoffice.html#exercises",
    "href": "qmd/01-pivot-tables-with-libreoffice.html#exercises",
    "title": "01-Discharge of River Elbe: Data Management with LibreOffice",
    "section": "3 Exercises",
    "text": "3 Exercises\n\n3.1 1. Download the data and inspect it\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.2 2. Create date categories\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.3 3. Aggregate data\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.4 4. Average year plot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.5 5. Annual discharge sum\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.6 6. Additional explorations\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.7 7. Cumulative sum per year (optional)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "qmd/03-discharge-elbe-project.html",
    "href": "qmd/03-discharge-elbe-project.html",
    "title": "x03-Discharge of River Elbe: Homework Project",
    "section": "",
    "text": "The project is related to the lab exercise “Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R”."
  },
  {
    "objectID": "qmd/03-discharge-elbe-project.html#outline",
    "href": "qmd/03-discharge-elbe-project.html#outline",
    "title": "x03-Discharge of River Elbe: Homework Project",
    "section": "3.1 Outline",
    "text": "3.1 Outline\nCombine all tasks together and tell a story, using a standard scientific outline, the so-called IMRAD scheme:\n\nIntroduction\nMethods\nResults\nDiscussion\nReferences\n\nPlease consult Wikipedia for a detailed explanation.\nAs it is a tiny report, Methods and Results may be merged in this case. However, Introduction and Discussion must be separated. Use the internet and find about 2-3 literature references for the Discussion."
  },
  {
    "objectID": "qmd/03-discharge-elbe-project.html#workflow",
    "href": "qmd/03-discharge-elbe-project.html#workflow",
    "title": "x03-Discharge of River Elbe: Homework Project",
    "section": "3.2 Workflow",
    "text": "3.2 Workflow\n\nDraft: draft your report. The first draft is usually somewhat longer.\nRefine: Discuss and select only the most important parts, and create the final version adhering to the page limit.\n\nCommunicate in your team, with other teams and with tutors\n\nPrimary Goal: Communication should promote community learning. Post approaches and specific questions in the Matrix^Matrix chat group so everyone can benefit.\nTeamwork: Discuss ideas within your team and with other colleagues first. Private channels for teamwork are allowed.\nIn the chatroom, please formulate specific questions (e.g., “How to format the numbers on a log-transformed axis?”) and avoid asking only, “Is this correct?”\nContribute: Actively contribute to answering your classmates’ questions!"
  },
  {
    "objectID": "qmd/03-discharge-elbe-project.html#report-formatting-instructions",
    "href": "qmd/03-discharge-elbe-project.html#report-formatting-instructions",
    "title": "x03-Discharge of River Elbe: Homework Project",
    "section": "3.3 Report formatting instructions",
    "text": "3.3 Report formatting instructions\nTo ensure clarity and efficiency, please adhere to the following strict page limits and formatting guidelines.\n\n3.3.1 Page limits and content focus\n\nCore Content Limit: The main body of the report (Introduction, Methods & Results, Discussion) must not exceed 4 A4 pages.\nThis limit forces you to distill the essential messages and choose only the most important figures.\nThe Title Page (Cover Sheet) and the List of References do not count towards the 4-page limit.\nQuality First: The goal is Quality instead of quantity! Use the limited space to focus on the interpretation and discussion of your findings.\n\n\n\n3.3.2 Text and visual balance\n\nThe report must have a good balance between explanatory text and supporting figures/tables.\nAvoid reports that are dominated by either large amounts of text or excessive, unexplained graphics.\nSelectivity: Only include figures and statistical output that are essential to support your claims in the text. Avoid “dumping” unnecessary output.\n\n\n\n3.3.3 Readability and citation standards\n\nUse a font size of 11 or 12 points.\nA line spacing of 1.2 lines is recommended to improve readability.\nFigures: font size of annotations must be well readable.\nCitation: Cite literature properly using the author-year style. Good examples can be found at the APA style web page."
  },
  {
    "objectID": "qmd/03-discharge-elbe-project.html#submission",
    "href": "qmd/03-discharge-elbe-project.html#submission",
    "title": "x03-Discharge of River Elbe: Homework Project",
    "section": "3.4 Submission",
    "text": "3.4 Submission\nYou will have 2.5 weeks time for the preparation of the report. Then upload it as PDF or HTML document and (optionally) your .R or Quarto (.qmd) scripts to the File folder of your group in the OPAL learning management system. Submissions after the deadline cannot be considered."
  },
  {
    "objectID": "qmd/04-distributions-leaves.html",
    "href": "qmd/04-distributions-leaves.html",
    "title": "04-Distribution and Confidence Intervals of Maple Leaf Samples",
    "section": "",
    "text": "The example aims to demonstrate estimation and interpretation of prediction intervals and confidence intervals. At the end, the two samples are compared with respect to variance and mean values.\nThe experimental hypotheses is, that the sampling strategy has an influence on the parameters of the distribution, i.e. that a sampling bias may occur. Here we leave it open, if the “subjective sampling” strategy prefers bigger or smaller leaves or if it has an influence on variance. The result is to be visualized with bar charts and box plots. We use the leave width as an example, an analysis of the other variables is left as an optional exercise.\nWe can now derive the following statistical hypotheses about the variance:\n\n\\(H_0\\): The variance of both samples is the same.\n\\(H_a\\): The samples have different variance.\n\nand about the mean:\n\n\\(H_0\\): The mean of both samples is the same.\n\\(H_a\\): The mean values of the samples are different."
  },
  {
    "objectID": "qmd/04-distributions-leaves.html#prepare-and-inspect-data",
    "href": "qmd/04-distributions-leaves.html#prepare-and-inspect-data",
    "title": "04-Distribution and Confidence Intervals of Maple Leaf Samples",
    "section": "3.1 Prepare and inspect data",
    "text": "3.1 Prepare and inspect data\nThe data set is available from your local learning management system (LMS, e.g. OPAL at TU Dresden) or publicly from https://tpetzoldt.github.io/datasets/data/leaves.csv.\n\nDownload the data set leaves.csv and use one of RStudio’s “Import Dataset” wizards.\nAlternative: use read.csv().\n\n\n#  ... do it\n\n\nplot everything, just for testing:\n\n\nplot(leaves)\n\n\nFirst, let’s apply a traditional approach and split leaves in two separate tables for the samples HSE and MHYB:\n\n\nhyb &lt;- subset(leaves, group == \"HYB\")\nhse &lt;- subset(leaves, group == \"HSE\")\n\n\nThen, compare leaf width of both groups graphically:\n\n\nboxplot(hse$width, hyb$width, names=c(\"HSE\", \"HYB\"))"
  },
  {
    "objectID": "qmd/04-distributions-leaves.html#check-distribution",
    "href": "qmd/04-distributions-leaves.html#check-distribution",
    "title": "04-Distribution and Confidence Intervals of Maple Leaf Samples",
    "section": "3.2 Check distribution",
    "text": "3.2 Check distribution\n\n# use `hist`, `qqnorm`, `qqline`\n# ..."
  },
  {
    "objectID": "qmd/04-distributions-leaves.html#sample-statistics-and-prediction-interval",
    "href": "qmd/04-distributions-leaves.html#sample-statistics-and-prediction-interval",
    "title": "04-Distribution and Confidence Intervals of Maple Leaf Samples",
    "section": "3.3 Sample statistics and prediction interval",
    "text": "3.3 Sample statistics and prediction interval\nIn a first analysis, we want to estimate the interval that covers 95% of leaves, defined by their width. As a first method, we take the empirical quantiles directly from the data. The method is also called “nonparametric,” because we don’t calculate mean and standard deviation and do not assume a normal or any other distribution.\n\nquantile(hse$width, p = c(0.025, 0.975))\n\nNow, we compare this empirical result with a method that relies on a specific distribution. If our initial graphical visualization (e.g., the histogram) suggests the data is reasonably symmetric and bell-shaped, we can proceed with a parametric assumption.\nWe first calculate mean, sd, N and se for “hse” data set:\n\nhse.mean &lt;- mean(hse$width)\nhse.sd   &lt;- sd(hse$width)\nhse.N    &lt;- length(hse$width)\nhse.se   &lt;- hse.sd/sqrt(hse.N)\n\nThen we estimate an approximate two-sided 95% prediction interval (\\(PI\\)) for the sample using a simplified approach based on the quantiles of the theoretical normal distribution (\\(z_{\\alpha/2} \\approx 1.96\\)) and the sample parameters mean \\(\\bar{x}\\) and standard deviation (\\(s\\)):\n\\[\nPI = \\bar{x} \\pm z \\cdot s\n\\]\nThis is the interval where we would predict a new single observation to fall with 95% confidence.\n\nhse.95 &lt;- hse.mean + c(-1.96, 1.96) * hse.sd\nhse.95\n\nInstead of using 1.96, we could also use the quantile function qnorm(0.975) for the upper interval or qnorm(c(0.025, 0.975)) for the lower and upper in parallel:\n\nhse.95 &lt;- hse.mean + qnorm(c(0.025, 0.975)) * hse.sd\nhse.95\n\nNow we plot the data and indicate the 95% interval:\n\nplot(hse$width)\nabline(h = hse.95, col=\"red\")\n\n… and the same as histogram:\n\nhist(hse$width)\nabline(v = hse.95, col=\"red\")\nrug(hse$width, col=\"blue\")"
  },
  {
    "objectID": "qmd/04-distributions-leaves.html#confidence-interval-of-the-mean",
    "href": "qmd/04-distributions-leaves.html#confidence-interval-of-the-mean",
    "title": "04-Distribution and Confidence Intervals of Maple Leaf Samples",
    "section": "3.4 Confidence interval of the mean",
    "text": "3.4 Confidence interval of the mean\nThe confidence interval (\\(CI\\)) of the mean tells us how precise a mean value was estimated from data. If the sample size is “large enough”, the distribution of the raw data does not necessarily need to be normal, because then mean values tend to approximate a normal distribution due to the central limit theorem.\nThe formula for the CI of the mean is: \\[CI = \\bar{x} \\pm t_{\\alpha/2, n-1} \\cdot \\frac{s}{\\sqrt{N}}\\]\n\n3.4.1 Confidence interval of the mean for the “hse” data\nTask: Calculate the confidence interval of the mean value of the “hse” data set using the quantile function (qt) of the t-distribution1:\n\nhse.ci &lt;- hse.mean + qt(p = c(0.025, 0.975), df = hse.N - 1) * hse.se\n\nNow indicate the confidence interval of the mean in the histogram.\n\nabline(v = hse.ci, col=\"magenta\")\n\n\n\n3.4.2 Confidence interval for the mean of the “hyb” data\n\n#  Do the same for the \"hyb\" data, calculate mean, sd, N, se and ci.\n# ...\n\n\n\n3.4.3 Discussion: Comparison and interpretation\nExplain the fundamental statistical reason why the 95% Prediction Interval (PI) for the leaf width is always significantly wider than the 95% Confidence Interval (CI) for the mean leaf width, even though both intervals are calculated from the same data set (hse)."
  },
  {
    "objectID": "qmd/04-distributions-leaves.html#comparison-of-the-samples",
    "href": "qmd/04-distributions-leaves.html#comparison-of-the-samples",
    "title": "04-Distribution and Confidence Intervals of Maple Leaf Samples",
    "section": "3.5 Comparison of the samples",
    "text": "3.5 Comparison of the samples\nTo compare the two samples. we already created box plots at the beginning. Instead of a boxplot, we can also use a bar chart with confidence intervals.\nThis can be done with the add-on package gplots (not to be confused with ggplot):\nSolution A) with package gplots\n\nlibrary(\"gplots\")\nbarplot2(height = c(hyb.mean, hse.mean),\n         ci.l   = c(hyb.ci[1], hse.ci[1]),\n         ci.u   = c(hyb.ci[2], hse.ci[2]),\n         plot.ci = TRUE,\n         names.arg=c(\"Hyb\", \"HSE\")\n)\n\nSolution B) without add-on packages (optional)\nHere we use a standard bar chart, and line segments for the error bars. One small problem arises, because barplot creates an own x-scaling. The good news is, that barplot returns its x-scale. We can store it in a variable, e.g. x that can then be used in subsequent code.\n\nx &lt;- barplot(c(hyb.mean, hse.mean),\n  names.arg=c(\"HYB\", \"HSE\"), ylim=c(0, 150))\nsegments(x0=x[1], y0=hyb.ci[1], y1=hyb.ci[2], lwd=2)\nsegments(x0=x[2], y0=hse.ci[1], y1=hse.ci[2], lwd=2)"
  },
  {
    "objectID": "qmd/04-distributions-leaves.html#is-the-difference-between-the-samples-statistically-significant",
    "href": "qmd/04-distributions-leaves.html#is-the-difference-between-the-samples-statistically-significant",
    "title": "04-Distribution and Confidence Intervals of Maple Leaf Samples",
    "section": "3.6 Is the difference between the samples statistically significant?",
    "text": "3.6 Is the difference between the samples statistically significant?\nIn the following, we compare the two samples with t- and F-Tests.\nHypotheses:\n\\(H_0\\): Both samples have the same mean width and variance.\n\\(H_A\\): The mean width (and possibly also the variance) differ because of more subjective sampling of HSE students. They may have prefered bigger or the nice small leaves.\n\nt.test(width ~ group, data = leaves)\n\nPerform also the classical t-test (var.equal=TRUE) and the F-test (var.test). Calculate absolute and relative effect size (mean differences) and interpret the results of all 3 tests.\n\n# var.test(...)\n# t.test(...)\n# ..."
  },
  {
    "objectID": "qmd/04-distributions-leaves.html#calculation-of-summary-statistics-with-dplyr",
    "href": "qmd/04-distributions-leaves.html#calculation-of-summary-statistics-with-dplyr",
    "title": "04-Distribution and Confidence Intervals of Maple Leaf Samples",
    "section": "4.1 Calculation of summary statistics with dplyr",
    "text": "4.1 Calculation of summary statistics with dplyr\n\nlibrary(\"dplyr\")\nleaves &lt;- read.csv(\"leaves.csv\")\n\nstats &lt;-\n  leaves |&gt;\n    group_by(group) |&gt;\n    summarize(mean = mean(width), sd = sd(width),\n              N = length(width), se = sd/sqrt(N),\n              lwr = mean + qt(p = 0.025, df = N-1) * se,\n              upr = mean + qt(p = 0.975, df = N-1) * se\n             )\n\nstats"
  },
  {
    "objectID": "qmd/04-distributions-leaves.html#barchart-and-errorbars-with-ggplot2",
    "href": "qmd/04-distributions-leaves.html#barchart-and-errorbars-with-ggplot2",
    "title": "04-Distribution and Confidence Intervals of Maple Leaf Samples",
    "section": "4.2 Barchart and errorbars with ggplot2",
    "text": "4.2 Barchart and errorbars with ggplot2\n\nlibrary(\"ggplot2\")\nstats |&gt;\n  ggplot(aes(x=group, y=mean, min=lwr, max=upr))  +\n    geom_col() + geom_errorbar(width=0.2)"
  },
  {
    "objectID": "qmd/04-distributions-leaves.html#a-footnote-about-prediction-intervals",
    "href": "qmd/04-distributions-leaves.html#a-footnote-about-prediction-intervals",
    "title": "04-Distribution and Confidence Intervals of Maple Leaf Samples",
    "section": "4.3 A footnote about prediction intervals",
    "text": "4.3 A footnote about prediction intervals\nThe simplified \\(\\bar{x} \\pm z \\cdot s\\) formula used above is an approximation. A statistically rigorous 95% prediction interval, especially for smaller samples, needs two corrections.\nFirst, we would use the t-distribution with the quantile \\(t_{\\alpha/2, n-1}\\) (or qt(alpha/2, n-1) in R) instead of the normal quantiles (\\(\\pm 1.96\\)). Then we add a term \\(\\sqrt{1+1/N}\\) that corrects for the sample parameters. The full formula for a single future observation is then:\n\\[\\text{PI} = \\bar{x} \\pm t_{\\alpha/2, n-1} \\cdot s \\cdot \\sqrt{1 + \\frac{1}{N}}\\]\nThe prediction interval is related to the so-called “tolerance interval”. Both are the same if the population parameters \\(\\mu, \\sigma\\) are known or the sample size is very big. However, there are theoretical and practical differences in case of small sample size."
  },
  {
    "objectID": "qmd/04-distributions-leaves.html#footnotes",
    "href": "qmd/04-distributions-leaves.html#footnotes",
    "title": "04-Distribution and Confidence Intervals of Maple Leaf Samples",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nas the sample size is not too small, you may also compare this with 1.96 or 2.0↩︎"
  },
  {
    "objectID": "qmd/06-classical-tests.html",
    "href": "qmd/06-classical-tests.html",
    "title": "06-Classical Tests",
    "section": "",
    "text": "The following exercises demonstrate some of the most common classical tests by means of simple examples.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#preface",
    "href": "qmd/06-classical-tests.html#preface",
    "title": "06-Classical Tests",
    "section": "Preface",
    "text": "Preface\nThe following exercises demonstrate some of the most common classical tests by means of simple examples.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#a-sleep-duration-study-statistical-tests-of-location",
    "href": "qmd/06-classical-tests.html#a-sleep-duration-study-statistical-tests-of-location",
    "title": "06-Classical Tests",
    "section": "A sleep duration study: statistical tests of location",
    "text": "A sleep duration study: statistical tests of location\nThe example is inspired by a classical test data set (Student, 1908) about a study with two groups of persons treated with two different pharmaceutical drugs.\nDrug 1: 8.7, 6.4, 7.8, 6.8, 7.9, 11.4, 11.7, 8.8, 8, 10\nDrug 2: 9.9, 8.8, 9.1, 8.1, 7.9, 12.4, 13.5, 9.6, 12.6, 11.4\nThe data are the duration of sleeping time in hours. It is assumed that the normal sleeping time would be 8 hours.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#one-sample-t-test",
    "href": "qmd/06-classical-tests.html#one-sample-t-test",
    "title": "06-Classical Tests",
    "section": "One sample t-Test",
    "text": "One sample t-Test\nLet’s test whether the drugs increased or decreased sleeping time, compared to 8 hours:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#two-sample-t-test",
    "href": "qmd/06-classical-tests.html#two-sample-t-test",
    "title": "06-Classical Tests",
    "section": "Two sample t-Test",
    "text": "Two sample t-Test\nThe two sample t-Test is used to compare two groups of data: Related to our example, we test the following hypotheses:\n\\(H_0\\): Both drugs have the same effect.\n\\(H_A\\): The drugs have a different effect, i.e. one of the drugs is stronger.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#paired-t-test",
    "href": "qmd/06-classical-tests.html#paired-t-test",
    "title": "06-Classical Tests",
    "section": "Paired t-test",
    "text": "Paired t-test\nGiven is a number of students that passed an examination in statistics. The examination was written two times, one time before and one time after an additional series of lectures. The values represent the numbers of points approached during the examination. Check whether the additional lectures had any positive effect:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#wilcoxon-test-optional",
    "href": "qmd/06-classical-tests.html#wilcoxon-test-optional",
    "title": "06-Classical Tests",
    "section": "Wilcoxon test (optional)",
    "text": "Wilcoxon test (optional)\nThe Mann-Whitney and Wilxon tests are nonparametric tests of location. “Nonparametric” means, that the general location of the distributions is compared and not a parameter like the mean. This makes the test independent of distributional assumptions, but can sometimes lead to a vague interpretations.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#own-project-weight-of-clementine-fruits",
    "href": "qmd/06-classical-tests.html#own-project-weight-of-clementine-fruits",
    "title": "06-Classical Tests",
    "section": "Own project: Weight of Clementine fruits",
    "text": "Own project: Weight of Clementine fruits\nImport the Clementines data set (fruits-2023-hse.csv)1.\n\nThink about an appropriate data structure and use a suitable statistical test to compare the weights.\nCheck variance homogeneity and normal distribution graphically.\nCan the weights from each brand be considered as independent samples?\n\nfruits.csv available from: https://tpetzoldt.github.io/datasets/data/fruits-2023-hse.csv",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#chi-squared-test-and-fishers-exact-test-optional",
    "href": "qmd/06-classical-tests.html#chi-squared-test-and-fishers-exact-test-optional",
    "title": "06-Classical Tests",
    "section": "Chi-squared test and Fisher’s exact test (optional)",
    "text": "Chi-squared test and Fisher’s exact test (optional)\nIntroduction\nTaken from Agresti (2002), Fisher’s Tea Drinker:\n“A British woman claimed to be able to distinguish whether milk or tea was added to the cup first. To test, she was given 8 cups of tea, in four of which milk was added first. The null hypothesis is that there is no association between the true order of pouring and the woman’s guess, the alternative that there is a positive association (that the odds ratio is greater than 1).”",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#exercise-solutions",
    "href": "qmd/06-classical-tests.html#exercise-solutions",
    "title": "06-Classical Tests",
    "section": "Exercise Solutions",
    "text": "Exercise Solutions",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#references",
    "href": "qmd/06-classical-tests.html#references",
    "title": "06-Classical Tests",
    "section": "References",
    "text": "References\n\nStudent (1908) - The probable error of a mean\nDelacre et al. (2017) - Why psychologists should by default use Welch’s t-test instead of Student’s t-test\n\n\n\n\n\nDelacre, M., Lakens, D., & Leys, C. (2017). Why psychologists should by default use Welch’s t-test instead of Student’s t-test. International Review of Social Psychology, 30(1), 92–101. https://doi.org/10.5334/irsp.82\n\n\nStudent. (1908). The probable error of a mean. Biometrika, 6(1), 1–25. https://doi.org/10.2307/2331554",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/06-classical-tests.html#footnotes",
    "href": "qmd/06-classical-tests.html#footnotes",
    "title": "06-Classical Tests",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nfruits.csv available from: https://tpetzoldt.github.io/datasets/data/fruits-2023-hse.csv↩︎",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/08-vollenweider-chl-tp.html",
    "href": "qmd/08-vollenweider-chl-tp.html",
    "title": "08-Linear Regression: Dependency of Chlorophyll on Phosphorus in Lakes",
    "section": "",
    "text": "The following example is based on the classical OECD study on “Eutrophication of Inland Waters” from Vollenweider & Kerekes (1980), that, among others, describes the relationship between annual mean chlorophyll concentration and annual mean total phosphorus in lakes. A website about aspects of this study can be found on https://www.chebucto.ns.ca/ccn/info/Science/SWCS/TPMODELS/OECD/oecd.html.\nWe use a data set that was taken manually from the figure below. It is of course not exactly the original data set, but should be sufficient for our purpose.\n\n\n\n\nDependency of chlorophyll a on total phosphorus (Reproduced Figure 6.1 from Vollenweider and Kerekes 1980)\n\n\nThis data set contains annual average concentrations of total phosphorus (TP, \\(\\mathrm{\\mu g L^{-1}}\\)) and chlorophyll a (CHLa, \\(\\mathrm{\\mu g L^{-1}}\\)) of 92 lakes. A few points were overlapping on the original figure, so that 2 lakes are missing.\n\n\nDownload oecd.csv from the course web1 page and copy it to a suitable folder. The first row contains the variable names (header=TRUE) and don’t forget to set the working directory to the correct location:\n\ndat &lt;- read.csv(\"oecd.csv\")\n\nNow, inspect the data set in RStudio. The columns contain an ID number (No), phosphorus (TP) and chlorophyll (CHLa) concentration and a last column indicating the limitation type of the lake: P: phosphorus limitation, N: nitrogen limitation and I: light limitation.\n\n\n\nFirst we want to know how much chlorophyll and phosphorus depend on each other. For this purpose, we calculate the Pearson and Spearman correlations and test it for significance:\n\nplot(dat$TP, dat$CHLa)\ncor.test(dat$TP, dat$CHLa)\ncor.test(dat$TP, dat$CHLa, method=\"spearman\")\ncor.test(rank(dat$TP), rank(dat$CHLa))\n\nCompare the values and discuss the results. The last line is just an alternative way to estimate Spearman correlation if ties (several times the same value) occur.\n\n\n\nNow lets fit a linear regression (lm means linear model) and save the result into a new object reg. This object can now be used to plot the regression line (abline) or to extract regression statistics (summary):\n\nplot(dat$TP, dat$CHL)\nreg &lt;- lm(dat$CHLa ~ dat$TP)\nabline(reg)\nsummary(reg)"
  },
  {
    "objectID": "qmd/08-vollenweider-chl-tp.html#methods",
    "href": "qmd/08-vollenweider-chl-tp.html#methods",
    "title": "08-Linear Regression: Dependency of Chlorophyll on Phosphorus in Lakes",
    "section": "",
    "text": "Download oecd.csv from the course web1 page and copy it to a suitable folder. The first row contains the variable names (header=TRUE) and don’t forget to set the working directory to the correct location:\n\ndat &lt;- read.csv(\"oecd.csv\")\n\nNow, inspect the data set in RStudio. The columns contain an ID number (No), phosphorus (TP) and chlorophyll (CHLa) concentration and a last column indicating the limitation type of the lake: P: phosphorus limitation, N: nitrogen limitation and I: light limitation."
  },
  {
    "objectID": "qmd/08-vollenweider-chl-tp.html#correlation-coefficient",
    "href": "qmd/08-vollenweider-chl-tp.html#correlation-coefficient",
    "title": "08-Linear Regression: Dependency of Chlorophyll on Phosphorus in Lakes",
    "section": "",
    "text": "First we want to know how much chlorophyll and phosphorus depend on each other. For this purpose, we calculate the Pearson and Spearman correlations and test it for significance:\n\nplot(dat$TP, dat$CHLa)\ncor.test(dat$TP, dat$CHLa)\ncor.test(dat$TP, dat$CHLa, method=\"spearman\")\ncor.test(rank(dat$TP), rank(dat$CHLa))\n\nCompare the values and discuss the results. The last line is just an alternative way to estimate Spearman correlation if ties (several times the same value) occur."
  },
  {
    "objectID": "qmd/08-vollenweider-chl-tp.html#linear-regression",
    "href": "qmd/08-vollenweider-chl-tp.html#linear-regression",
    "title": "08-Linear Regression: Dependency of Chlorophyll on Phosphorus in Lakes",
    "section": "",
    "text": "Now lets fit a linear regression (lm means linear model) and save the result into a new object reg. This object can now be used to plot the regression line (abline) or to extract regression statistics (summary):\n\nplot(dat$TP, dat$CHL)\nreg &lt;- lm(dat$CHLa ~ dat$TP)\nabline(reg)\nsummary(reg)"
  },
  {
    "objectID": "qmd/08-vollenweider-chl-tp.html#repeat-the-analysis-with-log-transformed-data",
    "href": "qmd/08-vollenweider-chl-tp.html#repeat-the-analysis-with-log-transformed-data",
    "title": "08-Linear Regression: Dependency of Chlorophyll on Phosphorus in Lakes",
    "section": "2.1 Repeat the analysis with log transformed data",
    "text": "2.1 Repeat the analysis with log transformed data\nThe results indicate that one of the most important pre-requisites of linear regression, namely “variance homogeneity” was violated. This can be seen from the fan-shaped pattern where most of the data points are found in the lower left corner. A logarithmic transformation of both variables can help here, so we should repeat the analysis with the logarithms of TP and CHLa.\n\nx &lt;- log(dat$TP)\ny &lt;- log(dat$CHLa)\nplot(x, y)\nreg &lt;- lm(y ~x)\nabline(reg)\nsummary(reg)"
  },
  {
    "objectID": "qmd/08-vollenweider-chl-tp.html#confidence-intervals",
    "href": "qmd/08-vollenweider-chl-tp.html#confidence-intervals",
    "title": "08-Linear Regression: Dependency of Chlorophyll on Phosphorus in Lakes",
    "section": "2.2 Confidence intervals",
    "text": "2.2 Confidence intervals\nConfidence and prediction intervals are useful to demonstrate uncertainty of the regression line and for predictions. The following code shows an example with test data.\n\nx &lt;- 1:10\ny &lt;- 2 + 3 * x + rnorm(10, sd=2)\nplot(x, y)\nreg &lt;- lm(y ~ x)\n\nnewdata &lt;- data.frame(x=seq(min(x), max(x), length=100))\nconflim &lt;- predict(reg, newdata=newdata, interval=\"confidence\")\npredlim &lt;- predict(reg, newdata=newdata, interval=\"prediction\")\nlines(newdata$x, conflim[,2], col=\"blue\", lty=\"dashed\") \nlines(newdata$x, conflim[,3], col=\"blue\", lty=\"dashed\")\nlines(newdata$x, predlim[,2], col=\"red\", lty=\"solid\") \nlines(newdata$x, predlim[,3], col=\"red\", lty=\"solid\")\n\nabline(reg)\n\n\n\n\n\n\n\n\nThe result of predict is a matrix with 3 columns:\n\n\n       fit      lwr      upr\n1 4.785678 1.334451 8.236906\n2 5.052683 1.650848 8.454519\n3 5.319688 1.966943 8.672434\n\n\n\nthe first column contains the fit,\nthe second and 3rd column the confidence resp. prediction intervals.\n\nYou can also find this out yourself by reading the documentation (?predict) or by inspecting conflim and predlim in the RStudio object explorer.\nThe data frame newdata contains \\(x\\) values for the prediction. Please note that the variable name (e.g. x or log_TP) must be exactly the same as in the lm-function!"
  },
  {
    "objectID": "qmd/08-vollenweider-chl-tp.html#course-task",
    "href": "qmd/08-vollenweider-chl-tp.html#course-task",
    "title": "08-Linear Regression: Dependency of Chlorophyll on Phosphorus in Lakes",
    "section": "3.1 Course task",
    "text": "3.1 Course task\nCombine your solution for the log-transformed chlorophyll-phosphorus regression with the confidence interval example, to reproduce the appearance of OECD Figure. Discuss the results."
  },
  {
    "objectID": "qmd/08-vollenweider-chl-tp.html#self-study",
    "href": "qmd/08-vollenweider-chl-tp.html#self-study",
    "title": "08-Linear Regression: Dependency of Chlorophyll on Phosphorus in Lakes",
    "section": "3.2 Self study",
    "text": "3.2 Self study\n\nInform yourself about the background of eutrophication, the classical OECD study (Vollenweider & Kerekes, 1980) and recent developments. Read Section 6 and 6.1 of the OECD report and find additional references.\nDiscuss the results of the Exercises above and back-transform the double logarithmic equation to linear scale using the laws of logs on a sheet of paper:\n\n\\[\\begin{align}\n    \\log(y) & = a + b \\log(x) \\\\\n    y       & = \\dots\n\\end{align}\\]\n\nCompare the equation with the equation in Figure 6.1. of the OECD report.\nRepeat the analysis for the P-limited data only: add confidence intervals, back-transform the equation and compare it with the equation in Fig. 6.1 of the report\nOptional: The axes of the own plots are log-transformed, and the annotations show the log. This is not easy to read. Find a way to annotate the axes with the original (not log-transformed) values as in the original report and add grid lines.\n\nNote: if you need help, contact your learning team members or ask in the matrix chat."
  },
  {
    "objectID": "qmd/08-vollenweider-chl-tp.html#notes",
    "href": "qmd/08-vollenweider-chl-tp.html#notes",
    "title": "08-Linear Regression: Dependency of Chlorophyll on Phosphorus in Lakes",
    "section": "3.3 Notes",
    "text": "3.3 Notes\nDifference between corelation and regression\n\ncorrelation measures whether a linear or monotonous dependency exists\nregression describes the relationship with a mathematical model\n\nExample how to use a subset\nWe can create a plot with different plotting symbols, by making the plotting character pch dependent on the limitation type\n\nplot(dat$TP, dat$CHL, pch = dat$Limitation)\n\nAnother method using plotting symbols employs conversion from character to a factor and then to a numeric:\n\nplot(dat$TP, dat$CHL, pch = as.numeric(factor(dat$Limitation)))\n\nThe following shows one approach for subsetting direcly in the lm-function. It is of course also possible to use a separate subset function before calling lm.\n\nreg &lt;- lm(CHLa ~ TP, data = dat, subset = (Limitation == \"P\"))\n# ...\n\nReferences\n\n\n\nVollenweider, R. A., & Kerekes, J. (1980). OECD cooperative programme for monitoring of inland waters. (Eutrophication control) [Synthesis Report]. Organisation for Economic Co-operation and Development. https://www.chebucto.ns.ca/science/SWCS/TPMODELS/OECD/OECD1982.pdf"
  },
  {
    "objectID": "qmd/08-vollenweider-chl-tp.html#footnotes",
    "href": "qmd/08-vollenweider-chl-tp.html#footnotes",
    "title": "08-Linear Regression: Dependency of Chlorophyll on Phosphorus in Lakes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data set is available from https://tpetzoldt.github.io/datasets/data/oecd.csv. Students of TU Dresden find it also in the OPAL learning management system.↩︎"
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html",
    "href": "qmd/10-nonlinear-regression-solution.html",
    "title": "x10-Fit Nonlinear Models to Plankton Growth Data",
    "section": "",
    "text": "The growth rate of a population is a direct measure of fitness. Therefore, determination of growth rates is common in many disciplines of natural and human sciences, business and engineering: ecology, pharmacology, wastewater treatment, and economic growth. The following example gives a brief introduction, how growth models can be fitted wit R."
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html#data-set",
    "href": "qmd/10-nonlinear-regression-solution.html#data-set",
    "title": "x10-Fit Nonlinear Models to Plankton Growth Data",
    "section": "2.1 Data set",
    "text": "2.1 Data set\nThe example data set was taken from a growth experiment in a batch culture with Microcystis aeruginosa, a cyanobacteria (blue green algae) species. Details of the experiment can be found in Jähnichen et al. (2001).\n\n## time (t)\nx &lt;- c(0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20)\n## Algae cell counts (per ml)\ny &lt;- c(0.88, 1.02, 1.43, 2.79, 4.61, 7.12,\n       6.47, 8.16, 7.28, 5.67, 6.91) * 1e6"
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html#methods",
    "href": "qmd/10-nonlinear-regression-solution.html#methods",
    "title": "x10-Fit Nonlinear Models to Plankton Growth Data",
    "section": "2.2 Methods",
    "text": "2.2 Methods\nParametric models are fitted using nonlinear regression according to the method of least squares. Data analysis is performed using the R software of statistical computing and graphics (R Core Team, 2021) and the nls function from package stats. An additional analysis is performed with packages growthrates (Petzoldt, 2020) and FME (Soetaert & Petzoldt, 2010).\nTo get a suitable curve, we need a model that fits the data and that has identifiable parameters. In the following, we use the logistic growth model (Verhulst, 1838):\n\\[\nN = \\frac{K \\cdot N_0}{(N_0 + (K - N_0) \\cdot \\exp(-r \\cdot x))}\n\\]\nand the Baranyi-Roberts model (Baranyi & Roberts, 1994), explained later."
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html#nonlinear-regression-with-nls",
    "href": "qmd/10-nonlinear-regression-solution.html#nonlinear-regression-with-nls",
    "title": "x10-Fit Nonlinear Models to Plankton Growth Data",
    "section": "3.1 Nonlinear regression with “nls”",
    "text": "3.1 Nonlinear regression with “nls”\n\n3.1.1 Logistic Growth\nWe define now a used defined function for the logistic and this by plotting the function with the start values (blue line). Then we can use function nls (nonlinear least squares) to fit the model:\n\n## function definition\nf &lt;- function(x, r, K, N0) {K /(1 + (K/N0 - 1) * exp(-r *x))}\n\n## check of start values\nplot(x, yy, pch=16, xlab=\"time (days)\", ylab=\"algae (Mio cells)\")\nlines(x, f(x, r=r, K=max(yy), N0=yy[1]), col=\"blue\")\n\n## nonlinear regression\npstart &lt;- c(r=r, K=max(yy), N0=yy[1])\nfit_logistic   &lt;- nls(yy ~ f(x, r, K, N0), start = pstart, trace=FALSE)\n\nx1 &lt;- seq(0, 25, length = 100)\nlines(x1, predict(fit_logistic, data.frame(x = x1)), col = \"red\")\nlegend(\"topleft\",\n       legend = c(\"data\", \"start parameters\", \"fitted parameters\"),\n       col = c(\"black\", \"blue\", \"red\"),\n       lty = c(0, 1, 1),\n       pch = c(16, NA, NA))\n\n\n\n\n\n\n\nsummary(fit_logistic)\n\n\nFormula: yy ~ f(x, r, K, N0)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nr    0.5682     0.1686   3.371  0.00978 ** \nK    7.0725     0.4033  17.535 1.14e-07 ***\nN0   0.1757     0.1861   0.944  0.37271    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8118 on 8 degrees of freedom\n\nNumber of iterations to convergence: 14 \nAchieved convergence tolerance: 4.018e-06\n\n(Rsquared &lt;- 1 - var(residuals(fit_logistic))/var(yy))\n\n[1] 0.931732\n\n\nWe see that the fit converged and the red line approximates the data, but we can also see that the model fit is far below the data at the beginning. This will be improved in the next section.\n\n\n3.1.2 Baranyi-Roberts model\nThe logistic function assumes, that growth starts exponentially from the beginning and then approaches more and more saturation. In reality, organisms need often some time to adapt to new conditions, and we can observe a delay at the beginnig. This delay is called lag-phase. Several models exist to describe such behavior, where the Baranyi-Roberts model (Baranyi & Roberts, 1994) is one of the most commonly used. Its parameters are similar to the logistic function with one additional parameter \\(h_0\\) for the lag. Following its mathematical equation (not shown here), we can implement it a suser-defined function in R:\n\nbaranyi &lt;- function(x, r, K, N0, h0) {\n  A &lt;- x + 1/r * log(exp(-r * x) + exp(-h0) - exp(-r * x - h0))\n  y &lt;- exp(log(N0) + r * A - log(1 + (exp(r * A) - 1)/exp(log(K) - log(N0))))\n  y\n}\n\nIf we assume a lag time \\(h_0 = 2\\), we can try to fit it and compare it with the logistic model\n\npstart &lt;- c(r=0.5, K=7, N0=1, h0=2)\nfit_baranyi   &lt;- nls(yy ~ baranyi(x, r, K, N0, h0), start = pstart, trace=FALSE)\n\nplot(x, yy, pch=16, xlab=\"time (days)\", ylab=\"algae (Mio cells)\")\nlines(x1, predict(fit_logistic, data.frame(x = x1)), col = \"red\")\nlines(x1, predict(fit_baranyi, data.frame(x = x1)), col = \"forestgreen\", lwd=2)\n\nlegend(\"topleft\",\n       legend = c(\"data\", \"logistic model\", \"Baranyi-Roberts model\"),\n       col = c(\"black\", \"red\", \"forestgreen\"),\n       lty = c(0, 1, 1),\n       pch = c(16, NA, NA))\n\n\n\n\n\n\n\n\nIt is obvious, that it fits much better."
  },
  {
    "objectID": "qmd/10-nonlinear-regression-solution.html#growth-curve-fitting-with-r-package-growthrates",
    "href": "qmd/10-nonlinear-regression-solution.html#growth-curve-fitting-with-r-package-growthrates",
    "title": "x10-Fit Nonlinear Models to Plankton Growth Data",
    "section": "3.2 Growth curve fitting with R package “growthrates”",
    "text": "3.2 Growth curve fitting with R package “growthrates”\nAs growth curves are of fundamental importance in science and engineering, several R packages exist for this problem. Here we show one of these packages growthrates (Petzoldt, 2020). Details can be found in the package documentation.\n\n3.2.1 Maximum growth rate as steepest increase in log scale\nThe package contains a method “easy linear” to find the steepest linear increase. It is a fully automatic method employing linear regression and a search routine. Details of the algorithm are found in Hall et al. (2014).\nThe following shows the phase of steepest increase, the exponential phase, identified by linear regression using the data points with the steepest increase:\n\nlibrary(\"growthrates\")\npar(mfrow=c(1, 2))\nfit_easy &lt;- fit_easylinear(x, yy)\nplot(fit_easy, main=\"linear scale\")\nplot(fit_easy, log=\"y\", main=\"log scale\")\n\n\n\n\n\n\n\ncoef(fit_easy)\n\n       y0     y0_lm     mumax       lag \n0.8800000 0.5838576 0.2528382 1.6226381 \n\n\n\n\n3.2.2 Logistic growth\nNow we can take the start parameters from above and function fit_growthmodel using the grow_logistic function, that is pre-defined in the package. We can also use a specific plot function from the package\n\npstart &lt;- c(mumax=r, K=max(yy), y0=yy[1])\nfit_logistic2 &lt;- fit_growthmodel(grow_logistic, p=pstart, time=x, y=yy)\nplot(fit_logistic2)\n\n\n\n\n\n\n\n\n\n\n3.2.3 Baranyi-Roberts model\nWe see again that the model fits not very well at the beginning because of the lag phase. Therefore, we empoy again an extended model e.g. the Baranyi model.\nA start value for the lag phase parameter \\(h_0\\) can be approximated from the “easylinear” method:\n\ncoef(fit_easy)\n\n       y0     y0_lm     mumax       lag \n0.8800000 0.5838576 0.2528382 1.6226381 \n\nh0 &lt;- 0.25 * 1.66\n\npstart &lt;- c(mumax=0.5, K=max(yy), y0=yy[1], h0=h0)\nfit_baranyi2 &lt;- fit_growthmodel(grow_baranyi, p=pstart, time=x, y=yy)\nsummary(fit_baranyi2)\n\n\nParameters:\n      Estimate Std. Error t value Pr(&gt;|t|)    \nmumax   0.8477     0.3681   2.303   0.0547 .  \nK       6.9969     0.3499  19.999 1.96e-07 ***\ny0      0.9851     0.5250   1.876   0.1027    \nh0      4.1220     3.0894   1.334   0.2239    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7583 on 7 degrees of freedom\n\nParameter correlation:\n        mumax       K      y0      h0\nmumax  1.0000 -0.3607  0.4600  0.9635\nK     -0.3607  1.0000 -0.1030 -0.2959\ny0     0.4600 -0.1030  1.0000  0.6477\nh0     0.9635 -0.2959  0.6477  1.0000\n\n\nThe summary shows the parameter estimates, their standard error and a significance level. However, we should not take the significance stars too seriously here. If we would, for example, omit the “nonsignificant” parameters y0 and h0, or set it to zero, the models would not work anymore. We see that some parameters correlate, especially h0 and y0. This can, in principle, indicate identification problems, but this dod not happen here, fortunatly.\nFinally, we plot the results in both, linear and log scale:\n\npar(mfrow=c(1, 2))\nplot(fit_logistic2, ylim=c(0, 10), las=1)\nlines(fit_baranyi2, col=\"magenta\")\n\npoints(x, yy, pch=16, col=\"red\")\n\n## log scale\nplot(fit_logistic2, log=\"y\", ylim=c(0.2, 10), las=1)\npoints(x, yy, pch=16, col=\"red\")\nlines(fit_baranyi2, col=\"magenta\")\nlegend(\"bottomright\",\n       legend = c(\"data\", \"logistic model\", \"Baranyi model\"),\n       col = c(\"red\", \"blue\", \"magenta\"),\n       lty = c(0, 1, 1),\n       pch = c(16, NA, NA))"
  },
  {
    "objectID": "qmd/12-timeseries-trends.html",
    "href": "qmd/12-timeseries-trends.html",
    "title": "12-An introductory time series example",
    "section": "",
    "text": "The main scientific question of the following examples is the existence of a trend. However, most trend tests assume stationarity of the residuals, so the concept of stationarity is first introduced by means of two artificial data sets. Here we introduce the following concepts:\n\ntrend stationarity and difference stationarity\nautocorrelation and partial autocorrelation\ntest for stationarity\ntest for a monotonic trend\n\nThe general procedure should then be applied to two real data sets as an exercise. Please keep in mind that the main objective here is trend analysis. The concepts of stationarity and autocorrelation and the related tests are only used as pre-tests to check if simple trend tests are possible. Please note also the importance of the effect size, e.g. the temperature increase pear year.\nThe book of Kleiber & Zeileis (2008) contains an excellent explanation of the methods described here and is strongly recommended for further reading."
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#data-set",
    "href": "qmd/12-timeseries-trends.html#data-set",
    "title": "12-An introductory time series example",
    "section": "2.1 Data set",
    "text": "2.1 Data set\nThe data set “timeseries.txt” contains artificial data with specifically designed properties, similar to the TSP and DSP series in the tutorial https://tpetzoldt.github.io/elements/.\nThe data sets are available from https://tpetzoldt.github.io/datasets/. You can either download it locally or modify the code that the data are directly read from the web. Then convert it to time series objects (ts), to make their analysis easier.\n\ndat &lt;- read.csv(\"timeseries.csv\")\nTSP &lt;- ts(dat$TSP)\nDSP &lt;- ts(dat$DSP)\n\nIt is always a good idea to plot the data first.\n\npar(mfrow=c(1,2))\nplot(TSP)\nplot(DSP)"
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#autocorrelation-and-partial-autocorrelation",
    "href": "qmd/12-timeseries-trends.html#autocorrelation-and-partial-autocorrelation",
    "title": "12-An introductory time series example",
    "section": "2.2 Autocorrelation and partial autocorrelation",
    "text": "2.2 Autocorrelation and partial autocorrelation\nFirst, plot the autocorrelation (acf) and partial autocorrelation (pacf) of the DSP series:\n\npar(mfrow=c(1,2))\nacf(DSP)\npacf(DSP)\n\n\n\n\n\n\n\n\n… and interpret the results.\nThen plot the acf for both series, together with the autocorrelation of the differenced and residual time series:\n\npar(mfrow=c(2,3))\nacf(TSP)\nacf(diff(TSP))\nacf(residuals(lm(TSP~time(TSP))))\nacf(DSP)\nacf(diff(DSP))\nacf(residuals(lm(DSP~time(DSP))))\n\nHere, diff is used for differencing the time series i.e. to compute differences between consecutive values, while lm fits a linear regression from which residuals extracts the residuals.\nThe autocorrelation function acf can be used to identify specific patterns. A series is considered as approximately stationary, if all autocorrelations (except for \\(lag=0\\)) are “almost non-significant”.\nHint: Deconstruct the parenthetisized statements like acf(residuals(lm(TSP~time(TSP)))) into 3 separate lines to understand better what they do. Plot the data and the trend."
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#stationarity-test",
    "href": "qmd/12-timeseries-trends.html#stationarity-test",
    "title": "12-An introductory time series example",
    "section": "2.3 Stationarity test",
    "text": "2.3 Stationarity test\nThe Kwiatkowski-Phillips-Schmidt-Shin test checks directly for stationarity, where \\(H_0\\) may be either level stationarity or trend stationarity. Don’t get confused:\n\nlevel stationary is just the same as stationary, the additional “level” just makes it clearer.\nin contrast, trend stationary is essentially non-stationary, but can easily be made stationary by subtracting a trend, because the residuals are stationary.\nthe warning message of the KPSS test is normal and not an “error”, its just an information that the true p-value is either smaller or greater than the printed value.\n\n\nlibrary(\"tseries\")\nkpss.test(TSP, null=\"Level\") # instationary\nkpss.test(TSP, null=\"Trend\") # stationary after trend removal\nkpss.test(DSP, null=\"Level\") # instationary\nkpss.test(DSP, null=\"Trend\") # still instationary\nkpss.test(diff(DSP), null=\"Level\")"
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#mann-kendall-test-for-trends",
    "href": "qmd/12-timeseries-trends.html#mann-kendall-test-for-trends",
    "title": "12-An introductory time series example",
    "section": "2.4 Mann-Kendall test for trends",
    "text": "2.4 Mann-Kendall test for trends\nThis is now finally the main test.\n\nlibrary(\"Kendall\")\nMannKendall(TSP) # correct only for trend stationary time series\nMannKendall(DSP) # wrong, because time series was difference stationary"
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#trend-of-air-temperature",
    "href": "qmd/12-timeseries-trends.html#trend-of-air-temperature",
    "title": "12-An introductory time series example",
    "section": "3.1 Trend of air temperature",
    "text": "3.1 Trend of air temperature\nThe plot seems to show an increasing trend, especially since 1980.\n\nplot(Tair)\n\nTest of stationarity\nTest for stationarity for the original and the trend adjusted series graphically with acf and quantitatively with the KPSS test:\n\nkpss.test(Tair)\n\nWe see that \\(p &lt; 0.01\\) so it is not “level stationary”!\nBut if we allow for a trend:\n\nkpss.test(Tair, null=\"Trend\")\n\n… we get \\(p &gt; 0.05\\) i.e. it is trend stationary (stationary after trend removal). Therefore, a trend test is possible.\nTrend test\nWe use the Mann-Kendall test dirst, that tests for monotonous trends:\n\nMannKendall(Tair)\n\nNow we fit a linear model to find out how much the temperature increased per day during this time.\n\nm &lt;- lm(Tair ~ time(Tair))\nsummary(m)\nplot(Tair)\nabline(m, col=\"red\")\n\nNow test the assumptions. Firstly test that the residuals have no autocorrelation:\n\nacf(residuals(m))\n\nOptional Task: use additional diagnostics, e.g. plot residuals versus fitted or qqnorm(residuals(m)) to test for normal distribution. Write the results down and evaluate what they can tell us.\nQuestions:\n\nIs the trend significant?\nwhich of the used tests is the best to test for a trend?\nwhat does “monotonous” mean?\nWhat is the advantage of fitting a linear model?\nWhat is the purpose of checking autocorrelation of the residuals with acf?"
  },
  {
    "objectID": "qmd/12-timeseries-trends.html#stationarity-and-trend-of-water-temperature",
    "href": "qmd/12-timeseries-trends.html#stationarity-and-trend-of-water-temperature",
    "title": "12-An introductory time series example",
    "section": "3.2 Stationarity and trend of water temperature",
    "text": "3.2 Stationarity and trend of water temperature\nNow repeat the same for the water temperature data, interpret the results and write a short report. Read about limnology of stratified lakes in temperate climate zones and discuss reasons why the trend of water temperature is weaker or stronger than air temperature.\nScientific Questions\n\nWas there a significant trend in water and air temperature?\nHow much Kelvin (degrees centigrade) did the water temperature increase on average during this time?\nWas the trend of water weaker or stronger than for air temperature? Which lake-physical processes are responsible for this effect?"
  },
  {
    "objectID": "qmd/14-flood-risk.html",
    "href": "qmd/14-flood-risk.html",
    "title": "14-Extreme Value Estimation with Package FAmle",
    "section": "",
    "text": "Here, it is described how to fit distributions to a given hydrological data set. Our intention here is to provide an example how easy and powerful distribution fitting can be done in R. More information can be found in Rice(2003), Hogg(2004), Coles (2001) and in the help file of package FAmle (Aucoin, 2001). For the given example, a data set from the US Geological Survey (USGS, http://waterdata.usgs.gov/nwis will be employed. The dataset consists of annual maximum daily peakflows (ft3/s) that were observed at a hydrometric station located at River James (Columbia). First the packages and the data set is loaded, then it is tested for potential trends and autocorrelation\n\n## load required packages\nlibrary(\"FAmle\")\nlibrary(\"FAdist\")\nlibrary(\"MASS\")\nlibrary(\"zoo\")\nlibrary(\"readr\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\n\n\n## St James River, Columbia\njamesriver &lt;- read_csv(\"jamesriver.csv\", col_types = c(\"D\", \"n\"))\n\nflow &lt;- jamesriver$flow\n\npar(mfrow=c(1, 2))\nplot(jamesriver$date, jamesriver$flow, type=\"b\", cex=0.4, pch=19, cex.axis=0.75, xlab=\"Year\", ylab=\"Flow\",\nmain=\"James River\")\nlines(lowess(jamesriver), col=\"red\")\nacf(jamesriver$flow, main=\"\")\n\n\n\n\n\n\n\nFigure 1: Time series (left) and auto-correlation plot (right) the daily flow (in ft3/s data set. The red smoothed line corresponds to a lowess fit."
  },
  {
    "objectID": "qmd/14-flood-risk.html#introduction",
    "href": "qmd/14-flood-risk.html#introduction",
    "title": "14-Extreme Value Estimation with Package FAmle",
    "section": "",
    "text": "Here, it is described how to fit distributions to a given hydrological data set. Our intention here is to provide an example how easy and powerful distribution fitting can be done in R. More information can be found in Rice(2003), Hogg(2004), Coles (2001) and in the help file of package FAmle (Aucoin, 2001). For the given example, a data set from the US Geological Survey (USGS, http://waterdata.usgs.gov/nwis will be employed. The dataset consists of annual maximum daily peakflows (ft3/s) that were observed at a hydrometric station located at River James (Columbia). First the packages and the data set is loaded, then it is tested for potential trends and autocorrelation\n\n## load required packages\nlibrary(\"FAmle\")\nlibrary(\"FAdist\")\nlibrary(\"MASS\")\nlibrary(\"zoo\")\nlibrary(\"readr\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\n\n\n## St James River, Columbia\njamesriver &lt;- read_csv(\"jamesriver.csv\", col_types = c(\"D\", \"n\"))\n\nflow &lt;- jamesriver$flow\n\npar(mfrow=c(1, 2))\nplot(jamesriver$date, jamesriver$flow, type=\"b\", cex=0.4, pch=19, cex.axis=0.75, xlab=\"Year\", ylab=\"Flow\",\nmain=\"James River\")\nlines(lowess(jamesriver), col=\"red\")\nacf(jamesriver$flow, main=\"\")\n\n\n\n\n\n\n\nFigure 1: Time series (left) and auto-correlation plot (right) the daily flow (in ft3/s data set. The red smoothed line corresponds to a lowess fit."
  },
  {
    "objectID": "qmd/14-flood-risk.html#empirical-quantiles",
    "href": "qmd/14-flood-risk.html#empirical-quantiles",
    "title": "14-Extreme Value Estimation with Package FAmle",
    "section": "2 Empirical Quantiles",
    "text": "2 Empirical Quantiles\n\nhist(flow, probability=TRUE, xlab=\"flow (ft^3/s)\")\nrug(flow)\nlines(density(flow))\n\n\n\n\n\n\n\nFigure 2: Histogram and empirical density of peak discharge.\n\n\n\n\n\nIf the data series is long enough, one may be tempted to use empirical quantiles, i.e. model and parameter free extrapolation from the data. We use this value as a baseline for the comparison with the model derived quantiles:\n\nquantile(p=c(0.95, 0.99), flow)\n\n 95%  99% \n4589 6974"
  },
  {
    "objectID": "qmd/14-flood-risk.html#lognormal-distribution-with-2-parameters",
    "href": "qmd/14-flood-risk.html#lognormal-distribution-with-2-parameters",
    "title": "14-Extreme Value Estimation with Package FAmle",
    "section": "3 Lognormal Distribution with 2 Parameters",
    "text": "3 Lognormal Distribution with 2 Parameters\nThe Lognormal distribution is often regarded as a plausible model for this type of data. However, other distributions such as Weibull, Lognormal with three parameters, and Johnson distributions may provide better fitting results. We will try some of them. The parameters of the distribution are estimated using maximum likelihood by the mle function con tained in package “FAmle”, except for the Johnson distribution wich needs a different procedure. Parameters of the fitting can be obtained as follows. It is important to pay attention to goodness-of-fit parameters (log likelihood and AIC) which provide us information about how good the model explains the corresponding data set.\n\nfitLn2 &lt;- mle(x=flow, dist=\"lnorm\", start=c(0.1, 0.1))\nfitLn2\n\n-----------------------------------------\n       Maximum Likelihood Estimates\n-----------------------------------------\nData object:  flow \nDistribution:  lnorm \n\n--------- Parameter estimates -----------\n\n         meanlog.hat sdlog.hat\nEstimate       6.294    1.4878\nStd.err        0.186    0.1319\n\n---------- Goodness-of-Fit --------------\n\n log.like       aic        ad       rho \n-518.8617 1041.7234    0.9627    0.9884 \n-----------------------------------------\n\n\n\n## automatic diagnostic plots\nplot(x=fitLn2, ci=TRUE, alpha=0.05)\n\n## which probability has a flow &gt;= 3000\n##  --&gt; two functions to provide the same result:\n\n### standard R function\nplnorm(3000, meanlog=fitLn2$par.hat[1], sdlog=fitLn2$par.hat[2])\n\n[1] 0.8751862\n\n### function from the FAmle package\ndistr(x=3000, dist=\"lnorm\", param=c(fitLn2$par.hat[1], fitLn2$par.hat[2]), type=\"p\")\n\n[1] 0.8751862\n\n## same for quantile (flow &gt;= 95% quantile)\nqlnorm(p=0.95, meanlog=fitLn2$par.hat[1], sdlog=fitLn2$par.hat[2])\n\n[1] 6252.526\n\ndistr(x=0.95, dist=\"lnorm\", param=c(fitLn2$par.hat[1], fitLn2$par.hat[2]), type=\"q\")\n\n[1] 6252.526\n\n## empirical quantile\nquantile(p=0.95, flow)\n\n 95% \n4589 \n\n\n\n\n\n\n\n\nFigure 3: Plot of the mle object corresponding to the fitting James River data using a Lognormal distribution\n\n\n\n\n\nThe function mle() provides also some goodness-of-fit statistics. This function creates a special kind of object which can be used inside of the standard R functions, e.g., plot(). A function called plot.mle may be used to generate a series of four diagnosis plots (Figure 3) for the mle object. Diagnostic plots for the model fitted to the dataset. The dashed red lines correspond to the lower and upper confidence bounds (definded by alpha) of the approximated 95% confidence intervals derived using the observed Fisher’s information matrix in conjunction with the so-called delta method.\nOnce the function is fitted to a distribution, these parameters can be used to calculate different quan- tiles. In this way we can find, for example, the value of the flow which has a probability lower than 5% or which is the probability of a flooding event of a certain flow.\nNow repeat for the 99% quantile\n…\nAnd extreme floods: 1% quantile\n…\nThe probability of a peakflow of 3000 ft3/s is obtained by either function “plnorm” or “distr” like follows:\n\nplnorm(3000, meanlog=fitLn2$par.hat[1], sdlog=fitLn2$par.hat[2], lower.tail=TRUE)\n\n[1] 0.8751862\n\ndistr(x=3000, dist=\"lnorm\", param=fitLn2$par.hat, type=\"p\")\n\n[1] 0.8751862"
  },
  {
    "objectID": "qmd/14-flood-risk.html#lognormal-distribution-with-3-parameters",
    "href": "qmd/14-flood-risk.html#lognormal-distribution-with-3-parameters",
    "title": "14-Extreme Value Estimation with Package FAmle",
    "section": "4 Lognormal Distribution with 3 Parameters",
    "text": "4 Lognormal Distribution with 3 Parameters\nLet’s repeat the procedure for a Lognormal distribution with three parameters. In this case the package FAdist is required. Results are presented in ?@fig-mle-ln3\n\n## Fit a lognormal distribution with three parameters\nfitLn3 &lt;- mle(x=flow, dist=\"lnorm3\", start=c(0.5, 0.5, 0.5))\nfitLn3\n\n-----------------------------------------\n       Maximum Likelihood Estimates\n-----------------------------------------\nData object:  flow \nDistribution:  lnorm3 \n\n--------- Parameter estimates -----------\n\n         shape.hat scale.hat thres.hat\nEstimate    1.4640    6.3065    -1.369\nStd.err     0.1552    0.1874     5.165\n\n---------- Goodness-of-Fit --------------\n\n log.like       aic        ad       rho \n-518.5289 1043.0578    0.8941    0.9891 \n-----------------------------------------\n\n## diagnostic plots\nhist(flow, probability=TRUE)\nrug(flow)\nlines(density(flow))\nfunLn3 &lt;- function(flow) distr(x=flow, model=fitLn3, type=\"d\")\ncurve(funLn3, add=TRUE, col=\"red\")\n\nplot(x=fitLn3, ci=TRUE, alpha=0.05)\n\n## theroretical and empirical quantiles\nqlnorm3(p=0.95, shape=fitLn3$par.hat[1], scale=fitLn3$par.hat[2], thres=fitLn3$par.hat[3])\n\n[1] 6089.576\n\ndistr(x=0.95, dist=\"lnorm3\", param=c(fitLn3$par.hat[1], fitLn3$par.hat[2], fitLn3$par.hat[3]), type=\"q\")\n\n[1] 6089.576\n\nquantile(p=0.95, flow)\n\n 95% \n4589 \n\n## Fit Weibull distribution to the data\nhist(flow, probability=TRUE)\nfitW &lt;- mle(x=flow, dist=\"weibull\", start=c(0.1, 0.1))\nfitW\n\n-----------------------------------------\n       Maximum Likelihood Estimates\n-----------------------------------------\nData object:  flow \nDistribution:  weibull \n\n--------- Parameter estimates -----------\n\n         shape.hat scale.hat\nEstimate   0.82050      1070\nStd.err    0.07811       172\n\n---------- Goodness-of-Fit --------------\n\n log.like       aic        ad       rho \n-515.2496 1034.4993    0.3602    0.9681 \n-----------------------------------------\n\n## diagnostics\nfunW &lt;- function(flow) distr(x=flow, model=fitW, type=\"d\")\ncurve(funW, add=TRUE, col=\"blue\")\n\nplot(x=fitW, ci=TRUE, alpha=0.05)\n\n## quantiles\nqweibull(p=0.99, shape=fitW$par.hat[1], scale=fitW$par.hat[2])\n\n[1] 6884.165\n\ndistr(x=0.99, dist=\"weibull\", param=c(fitW$par.hat[1], fitW$par.hat[2]), type=\"q\")\n\n[1] 6884.165\n\nquantile(p=0.99, flow)\n\n 99% \n6974 \n\n## Which distribution is the best according to the AIC?\nfitLn2$aic\n\n[1] 1041.723\n\nfitLn3$aic\n\n[1] 1043.058\n\nfitW$aic\n\n[1] 1034.499\n\n\n\n\n\n\n\n\nFigure 4: Plot of the mle object corresponding to the fitting James River data using a LN3 distribution\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Plot of the mle object corresponding to the fitting James River data using a LN3 distribution\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Plot of the mle object corresponding to the fitting James River data using a LN3 distribution\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Plot of the mle object corresponding to the fitting James River data using a LN3 distribution"
  },
  {
    "objectID": "qmd/14-flood-risk.html#exercise-extreme-values-of-the-elbe-river",
    "href": "qmd/14-flood-risk.html#exercise-extreme-values-of-the-elbe-river",
    "title": "14-Extreme Value Estimation with Package FAmle",
    "section": "5 Exercise: Extreme values of the Elbe river",
    "text": "5 Exercise: Extreme values of the Elbe river\nNow load the Elbe River data from the beginning of the course and note that we need annual maximum values.\n\nelbe &lt;- read_csv(\"https://raw.githubusercontent.com/tpetzoldt/datasets/main/data/elbe.csv\", , col_types = c(\"D\", \"n\"))\n\n\n## annual maximum discharge\nelbe_annual &lt;-\n  mutate(elbe, year = year(date)) |&gt;\n  group_by(year) |&gt;\n  summarize(discharge = max(discharge))\n\nplot(discharge ~ year, data = elbe_annual)\n\n## check for trend and autocorrelation between years\nMannKendall(elbe_annual$discharge)\nacf(elbe_annual$discharge)\n\n\nfitLn3 &lt;- mle(x=elbe_annual$discharge, dist=\"lnorm3\", start=c(1, 5, 100))\nfitLn3\n\nflow &lt;- elbe_annual$discharge\n\nhist(flow, probability=TRUE, breaks = 10)\n\nrug(flow)\nlines(density(flow))\n\nxnew &lt;- seq(min(flow), max(flow), length = 100)\nfunLn3 &lt;- function(flow) distr(x=flow, model=fitLn3, type=\"d\")\nlines(xnew, funLn3(xnew), col=\"red\")\n\nImportant: The method described so far assumes stationarity of conditions, i.e. absence of meteorological and hydrological trends. Discuss, how climate warming already influences validity of the described method, and which methods need to be applied instead."
  },
  {
    "objectID": "qmd/temp1.html",
    "href": "qmd/temp1.html",
    "title": "x03-Discharge of River Elbe: Homework Project",
    "section": "",
    "text": "The project is related to the lab exercise “Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R”."
  },
  {
    "objectID": "qmd/temp1.html#outline",
    "href": "qmd/temp1.html#outline",
    "title": "x03-Discharge of River Elbe: Homework Project",
    "section": "3.1 Outline",
    "text": "3.1 Outline\nCombine all tasks together and tell a story, using a standard scientific outline, the so-called IMRAD scheme:\n\nIntroduction\nMethods\nResults\nDiscussion\nReferences\n\nPlease consult Wikipedia for a detailed explanation.\nAs it is a tiny report, Methods and Results may be merged in this case. However, Introduction and Discussion must be separated. Use the internet and find about 2-3 literature references for the Discussion."
  },
  {
    "objectID": "qmd/temp1.html#workflow",
    "href": "qmd/temp1.html#workflow",
    "title": "x03-Discharge of River Elbe: Homework Project",
    "section": "3.2 Workflow",
    "text": "3.2 Workflow\n\nDraft: draft your report. The first draft is usually somewhat longer.\nRefine: Discuss and select only the most important parts, and create the final version adhering to the page limit.\n\nCommunicate in your team, with other teams and with tutors\n\nPrimary Goal: Communication should promote community learning. Post approaches and specific questions in the Matrix^Matrix chat group so everyone can benefit.\nTeamwork: Discuss ideas within your team and with other colleagues first. Private channels for teamwork are allowed.\nIn the chatroom, please formulate specific questions (e.g., “How to format the numbers on a log-transformed axis?”) and avoid asking only, “Is this correct?”\nContribute: Actively contribute to answering your classmates’ questions!"
  },
  {
    "objectID": "qmd/temp1.html#report-formatting-instructions",
    "href": "qmd/temp1.html#report-formatting-instructions",
    "title": "x03-Discharge of River Elbe: Homework Project",
    "section": "3.3 Report formatting instructions",
    "text": "3.3 Report formatting instructions\nTo ensure clarity and efficiency, please adhere to the following strict page limits and formatting guidelines.\n\n3.3.1 Page limits and content focus\n\nCore Content Limit: The main body of the report (Introduction, Methods & Results, Discussion) must not exceed 4 A4 pages.\nThis limit forces you to distill the essential messages and choose only the most important figures.\nThe Title Page (Cover Sheet) and the List of References do not count towards the 4-page limit.\nQuality First: The goal is Quality instead of quantity! Use the limited space to focus on the interpretation and discussion of your findings.\n\n\n\n3.3.2 Text and visual balance\n\nThe report must have a good balance between explanatory text and supporting figures/tables.\nAvoid reports that are dominated by either large amounts of text or excessive, unexplained graphics.\nSelectivity: Only include figures and statistical output that are essential to support your claims in the text. Avoid “dumping” unnecessary output.\n\n\n\n3.3.3 Readability and citation standards\n\nUse a font size of 11 or 12 points.\nA line spacing of 1.2 lines is recommended to improve readability.\nFigures: font size of annotations must be well readable.\nCitation: Cite literature properly using the author-year style. Good examples can be found at the APA style web page."
  },
  {
    "objectID": "qmd/temp1.html#submission",
    "href": "qmd/temp1.html#submission",
    "title": "x03-Discharge of River Elbe: Homework Project",
    "section": "3.4 Submission",
    "text": "3.4 Submission\nYou will have 2.5 weeks time for the preparation of the report. Then upload it as PDF or HTML document and (optionally) your .R or Quarto (.qmd) scripts to the File folder of your group in the OPAL learning management system. Submissions after the deadline cannot be considered."
  },
  {
    "objectID": "qmd/temp2.html#preface",
    "href": "qmd/temp2.html#preface",
    "title": "06-Classical Tests",
    "section": "Preface",
    "text": "Preface\nThe following exercises demonstrate some of the most common classical tests by means of simple examples.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/temp2.html#a-sleep-duration-study-statistical-tests-of-location",
    "href": "qmd/temp2.html#a-sleep-duration-study-statistical-tests-of-location",
    "title": "06-Classical Tests",
    "section": "A sleep duration study: statistical tests of location",
    "text": "A sleep duration study: statistical tests of location\nThe example is inspired by a classical test data set (Student, 1908) about a study with two groups of persons treated with two different pharmaceutical drugs.\nDrug 1: 8.7, 6.4, 7.8, 6.8, 7.9, 11.4, 11.7, 8.8, 8, 10\nDrug 2: 9.9, 8.8, 9.1, 8.1, 7.9, 12.4, 13.5, 9.6, 12.6, 11.4\nThe data are the duration of sleeping time in hours. It is assumed that the normal sleeping time would be 8 hours.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/temp2.html#one-sample-t-test",
    "href": "qmd/temp2.html#one-sample-t-test",
    "title": "06-Classical Tests",
    "section": "One sample t-Test",
    "text": "One sample t-Test\nLet’s test whether the drugs increased or decreased sleeping time, compared to 8 hours:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/temp2.html#two-sample-t-test",
    "href": "qmd/temp2.html#two-sample-t-test",
    "title": "06-Classical Tests",
    "section": "Two sample t-Test",
    "text": "Two sample t-Test\nThe two sample t-Test is used to compare two groups of data: Related to our example, we test the following hypotheses:\n\\(H_0\\): Both drugs have the same effect.\n\\(H_A\\): The drugs have a different effect, i.e. one of the drugs is stronger.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/temp2.html#paired-t-test",
    "href": "qmd/temp2.html#paired-t-test",
    "title": "06-Classical Tests",
    "section": "Paired t-test",
    "text": "Paired t-test\nGiven is a number of students that passed an examination in statistics. The examination was written two times, one time before and one time after an additional series of lectures. The values represent the numbers of points approached during the examination. Check whether the additional lectures had any positive effect:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/temp2.html#wilcoxon-test-optional",
    "href": "qmd/temp2.html#wilcoxon-test-optional",
    "title": "06-Classical Tests",
    "section": "Wilcoxon test (optional)",
    "text": "Wilcoxon test (optional)\nThe Mann-Whitney and Wilxon tests are nonparametric tests of location. “Nonparametric” means, that the general location of the distributions is compared and not a parameter like the mean. This makes the test independent of distributional assumptions, but can sometimes lead to a vague interpretations.",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/temp2.html#own-project-weight-of-clementine-fruits",
    "href": "qmd/temp2.html#own-project-weight-of-clementine-fruits",
    "title": "06-Classical Tests",
    "section": "Own project: Weight of Clementine fruits",
    "text": "Own project: Weight of Clementine fruits\nImport the Clementines data set (fruits-2023-hse.csv)1.\n\nThink about an appropriate data structure and use a suitable statistical test to compare the weights.\nCheck variance homogeneity and normal distribution graphically.\nCan the weights from each brand be considered as independent samples?\n\nfruits.csv available from: https://tpetzoldt.github.io/datasets/data/fruits-2023-hse.csv",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/temp2.html#chi-squared-test-and-fishers-exact-test-optional",
    "href": "qmd/temp2.html#chi-squared-test-and-fishers-exact-test-optional",
    "title": "06-Classical Tests",
    "section": "Chi-squared test and Fisher’s exact test (optional)",
    "text": "Chi-squared test and Fisher’s exact test (optional)\nIntroduction\nTaken from Agresti (2002), Fisher’s Tea Drinker:\n“A British woman claimed to be able to distinguish whether milk or tea was added to the cup first. To test, she was given 8 cups of tea, in four of which milk was added first. The null hypothesis is that there is no association between the true order of pouring and the woman’s guess, the alternative that there is a positive association (that the odds ratio is greater than 1).”",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/temp2.html#exercise-solutions",
    "href": "qmd/temp2.html#exercise-solutions",
    "title": "06-Classical Tests",
    "section": "Exercise Solutions",
    "text": "Exercise Solutions",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/temp2.html#references",
    "href": "qmd/temp2.html#references",
    "title": "06-Classical Tests",
    "section": "References",
    "text": "References\n\nStudent (1908) - The probable error of a mean\nDelacre et al. (2017) - Why psychologists should by default use Welch’s t-test instead of Student’s t-test\n\n\n\n\n\nDelacre, M., Lakens, D., & Leys, C. (2017). Why psychologists should by default use Welch’s t-test instead of Student’s t-test. International Review of Social Psychology, 30(1), 92–101. https://doi.org/10.5334/irsp.82\n\n\nStudent. (1908). The probable error of a mean. Biometrika, 6(1), 1–25. https://doi.org/10.2307/2331554",
    "crumbs": [
      "Labs",
      "06-Classical Tests"
    ]
  },
  {
    "objectID": "qmd/temp3.html",
    "href": "qmd/temp3.html",
    "title": "03-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R",
    "section": "",
    "text": "This practical example demonstrates how daily discharge data of the Elbe River can be analyzed and visualized in R directly in a browser using Web-R.\nScientific aim: Learn date/time computation, data management, aggregation, and plotting."
  },
  {
    "objectID": "qmd/temp3.html#introduction",
    "href": "qmd/temp3.html#introduction",
    "title": "03-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R",
    "section": "",
    "text": "This practical example demonstrates how daily discharge data of the Elbe River can be analyzed and visualized in R directly in a browser using Web-R.\nScientific aim: Learn date/time computation, data management, aggregation, and plotting."
  },
  {
    "objectID": "qmd/temp3.html#methods",
    "href": "qmd/temp3.html#methods",
    "title": "03-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R",
    "section": "2 Methods",
    "text": "2 Methods\nWe demonstrate data import, conversion, aggregation, plotting, and pipeline usage using the tidyverse. Data are daily measurements for the Elbe River (m³/s) from Dresden, provided by BfG."
  },
  {
    "objectID": "qmd/temp3.html#exercises",
    "href": "qmd/temp3.html#exercises",
    "title": "03-Discharge of River Elbe: Date and Time Computation, Data Management and Plotting with R",
    "section": "3 Exercises",
    "text": "3 Exercises\n\n3.1 1. Download the data and inspect it\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.2 2. Create date categories\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.3 3. Aggregate monthly data\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.4 4. Average year plot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.5 5. Annual discharge sum\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.6 6. Scatter plot per year\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.7 7. Cumulative sum per year\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.8 8. Min-Max Plot with ggplot2\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.9 9. Pivot table (wide ↔︎ long)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "qmd/temp4.html",
    "href": "qmd/temp4.html",
    "title": "Title",
    "section": "",
    "text": "flowchart TD\n    A[Christmas] --&gt;|Get money| B(Go shopping)\n    B --&gt; C{Let me think}\n    C --&gt;|One| D[Laptop]\n    C --&gt;|Two| E[iPhone]\n    \n\n\n\n\n\n\nPlanning and maintenance of waterways and rivers needs adequate measurements and data. Raw time series can often be long and confusing, so a first step is aggregation and visualization.\nScientific aim: plot an average year and calculate monthly averages, minima and maxima of the discharge."
  },
  {
    "objectID": "qmd/temp4.html#introduction",
    "href": "qmd/temp4.html#introduction",
    "title": "Title",
    "section": "",
    "text": "flowchart TD\n    A[Christmas] --&gt;|Get money| B(Go shopping)\n    B --&gt; C{Let me think}\n    C --&gt;|One| D[Laptop]\n    C --&gt;|Two| E[iPhone]\n    \n\n\n\n\n\n\nPlanning and maintenance of waterways and rivers needs adequate measurements and data. Raw time series can often be long and confusing, so a first step is aggregation and visualization.\nScientific aim: plot an average year and calculate monthly averages, minima and maxima of the discharge."
  },
  {
    "objectID": "qmd/temp4.html#methods",
    "href": "qmd/temp4.html#methods",
    "title": "Title",
    "section": "2 Methods",
    "text": "2 Methods\nThe approach demonstrates the use of pivot tables for aggregating data according to certain criteria. We will also use date and time computation to derive aggregation criteria from a single date column.\nThe data set consists of daily measurements for discharge of the Elbe River in Dresden (daily discharge sum in \\(\\mathrm{m^3 d^{-1}}\\)).\nData were kindly provided by the German Federal Institute for Hydrology (BfG)."
  },
  {
    "objectID": "qmd/temp4.html#exercises",
    "href": "qmd/temp4.html#exercises",
    "title": "Title",
    "section": "3 Exercises",
    "text": "3 Exercises\n\n3.1 1. Download the data and inspect it\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.2 2. Create date categories\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.3 3. Aggregate data\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.4 4. Average year plot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.5 5. Annual discharge sum\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.6 6. Additional explorations\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n3.7 7. Cumulative sum per year (optional)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html",
    "href": "qmd/01-analyse-profiles-github.html",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "",
    "text": "Ce document présente une analyse statistique complète des profils GitHub les plus influents à travers le monde. GitHub est la plateforme de référence pour le développement collaboratif, et le nombre d’étoiles (stars) récoltées par un utilisateur est devenu un indicateur clé de sa popularité et de son impact dans la communauté Open Source.\n\n\nNous tenterons de répondre aux questions suivantes :\n\nDistribution de la popularité : Comment la popularité est-elle distribuée entre ces différents profils ?\nImpact des langages : Le choix des langages de programmation (comme Python ou JavaScript) influence-t-il le succès d’un profil ?\nTypes de profils : Existe-t-il des différences notables entre les comptes individuels et les organisations ?\nDiversité technologique : La diversité des langages utilisés est-elle corrélée à la popularité ?",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#objectifs-de-lanalyse",
    "href": "qmd/01-analyse-profiles-github.html#objectifs-de-lanalyse",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "",
    "text": "Nous tenterons de répondre aux questions suivantes :\n\nDistribution de la popularité : Comment la popularité est-elle distribuée entre ces différents profils ?\nImpact des langages : Le choix des langages de programmation (comme Python ou JavaScript) influence-t-il le succès d’un profil ?\nTypes de profils : Existe-t-il des différences notables entre les comptes individuels et les organisations ?\nDiversité technologique : La diversité des langages utilisés est-elle corrélée à la popularité ?",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#installation-et-chargement-des-packages",
    "href": "qmd/01-analyse-profiles-github.html#installation-et-chargement-des-packages",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "2.1 Installation et chargement des packages",
    "text": "2.1 Installation et chargement des packages\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#lecture-du-fichier-de-données",
    "href": "qmd/01-analyse-profiles-github.html#lecture-du-fichier-de-données",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "2.2 Lecture du fichier de données",
    "text": "2.2 Lecture du fichier de données\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Le dataset contient 30 profils GitHub avec 7 variables principales. La structure des données révèle que chaque profil possède un identifiant unique (login), des informations personnelles (nom, entreprise, localisation), ainsi que des métriques quantitatives (total_stars, nb_repos_fetched) et qualitatives (languages_list). La lecture depuis une URL garantit que les données sont toujours à jour et accessibles.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#remplacement-des-valeurs-manquantes",
    "href": "qmd/01-analyse-profiles-github.html#remplacement-des-valeurs-manquantes",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "3.1 Remplacement des valeurs manquantes",
    "text": "3.1 Remplacement des valeurs manquantes\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Le remplacement des valeurs manquantes par “Unknown” permet de conserver tous les profils dans l’analyse sans perte d’information. Cette approche est appropriée car certaines informations peuvent simplement ne pas être renseignées par les utilisateurs GitHub plutôt que d’être réellement manquantes. Cela évite les biais liés à l’élimination des données incomplètes.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#création-de-variables-flags-pour-les-langages",
    "href": "qmd/01-analyse-profiles-github.html#création-de-variables-flags-pour-les-langages",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "3.2 Création de variables “flags” pour les langages",
    "text": "3.2 Création de variables “flags” pour les langages\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : La création de variables booléennes facilite l’analyse comparative entre profils utilisant ou non un langage donné. Ces variables permettent de tester si l’utilisation d’un langage spécifique est associée à une popularité différente. Python, JavaScript et TypeScript sont choisis car ce sont parmi les langages les plus populaires et demandés dans l’écosystème de développement moderne.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#statistiques-descriptives",
    "href": "qmd/01-analyse-profiles-github.html#statistiques-descriptives",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "4.1 Statistiques descriptives",
    "text": "4.1 Statistiques descriptives\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Les statistiques descriptives révèlent une importante variabilité dans le nombre d’étoiles, avec des valeurs extrêmes qui suggèrent une distribution asymétrique. La médiane et la moyenne du nombre d’étoiles sont probablement très différentes, indiquant la présence de quelques profils exceptionnellement populaires. Les tableaux de fréquences montrent la prévalence de chaque langage dans l’échantillon, permettant d’identifier les langages les plus représentés.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#distribution-du-nombre-total-détoiles",
    "href": "qmd/01-analyse-profiles-github.html#distribution-du-nombre-total-détoiles",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "4.2 Distribution du nombre total d’étoiles",
    "text": "4.2 Distribution du nombre total d’étoiles\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : L’histogramme révèle une distribution fortement asymétrique (skewed right) avec une queue longue vers les valeurs élevées. Cela signifie que la majorité des profils ont un nombre modéré d’étoiles, tandis qu’un petit nombre de profils concentrent une part disproportionnée de la popularité totale. Cette distribution suit probablement une loi de Pareto (loi des 80/20), caractéristique des systèmes sociaux où quelques acteurs dominent.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#impact-des-langages-sur-la-popularité",
    "href": "qmd/01-analyse-profiles-github.html#impact-des-langages-sur-la-popularité",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "4.3 Impact des langages sur la popularité",
    "text": "4.3 Impact des langages sur la popularité\n\n4.3.1 Python\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Le boxplot en échelle logarithmique permet de comparer visuellement la distribution des étoiles entre les deux groupes. Si les boîtes à moustaches sont à des hauteurs différentes, cela suggère un écart potentiel de popularité. L’échelle logarithmique est essentielle ici car elle compresse les très grandes différences et rend la visualisation lisible. Une médiane plus élevée pour le groupe “Python = TRUE” suggérerait que Python pourrait être associé à une plus grande popularité.\n\n\n4.3.2 JavaScript\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Similaire à l’analyse Python, ce graphique compare la popularité des profils utilisant JavaScript. JavaScript étant le langage le plus utilisé pour le développement web, on pourrait s’attendre à ce que les profils utilisant JavaScript soient plus nombreux. Cependant, cela ne garantit pas nécessairement une plus grande popularité moyenne, car la qualité et la visibilité des projets sont plus importantes que le simple choix technologique.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#test-de-wilcoxon-pour-python",
    "href": "qmd/01-analyse-profiles-github.html#test-de-wilcoxon-pour-python",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "5.1 Test de Wilcoxon pour Python",
    "text": "5.1 Test de Wilcoxon pour Python\nQuestion de recherche : Est-ce que les profils utilisant Python ont significativement plus d’étoiles que les autres ?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Le test de Wilcoxon (test non-paramétrique de Mann-Whitney) est choisi car il ne fait pas d’hypothèse sur la distribution normale des données, ce qui est approprié ici compte tenu de l’asymétrie observée. Si p &lt; 0.05, on rejette l’hypothèse nulle d’égalité des distributions et on conclut qu’il existe une différence statistiquement significative entre les deux groupes. Cependant, un résultat significatif ne signifie pas nécessairement une causalité - il pourrait s’agir d’une simple corrélation due à d’autres facteurs.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#test-de-wilcoxon-pour-javascript",
    "href": "qmd/01-analyse-profiles-github.html#test-de-wilcoxon-pour-javascript",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "5.2 Test de Wilcoxon pour JavaScript",
    "text": "5.2 Test de Wilcoxon pour JavaScript\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : De la même manière que pour Python, ce test évalue si l’utilisation de JavaScript est associée à une différence significative de popularité. Un résultat non significatif pourrait indiquer que JavaScript est si répandu qu’il n’apporte pas de distinction particulière, ou que d’autres facteurs (qualité du code, timing, marketing) sont plus déterminants pour le succès que le simple choix du langage.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#test-de-corrélation",
    "href": "qmd/01-analyse-profiles-github.html#test-de-corrélation",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "7.1 Test de corrélation",
    "text": "7.1 Test de corrélation\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Le test de corrélation de Spearman mesure la force de la relation monotone (pas nécessairement linéaire) entre les deux variables. Un coefficient proche de 0 indique l’absence de corrélation, tandis qu’une valeur proche de +1 ou -1 indique une forte corrélation positive ou négative. Le test de Spearman est préféré au test de Pearson car il est plus robuste aux valeurs aberrantes et ne nécessite pas d’hypothèse de normalité.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#calcul-du-nombre-de-langages-par-profil",
    "href": "qmd/01-analyse-profiles-github.html#calcul-du-nombre-de-langages-par-profil",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "8.1 Calcul du nombre de langages par profil",
    "text": "8.1 Calcul du nombre de langages par profil\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Le calcul du nombre de langages permet d’évaluer la diversité technologique de chaque profil. Un nombre élevé de langages pourrait indiquer une polyvalence technique, mais aussi une dispersion potentielle des efforts. L’histogramme montre la distribution de cette diversité dans l’échantillon, révélant si la plupart des profils se spécialisent (peu de langages) ou sont polyvalents (beaucoup de langages).",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#corrélation-entre-diversité-et-popularité",
    "href": "qmd/01-analyse-profiles-github.html#corrélation-entre-diversité-et-popularité",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "8.2 Corrélation entre diversité et popularité",
    "text": "8.2 Corrélation entre diversité et popularité\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Ce test évalue si la diversité technologique est un atout ou un handicap pour la popularité. Une corrélation positive suggérerait que la polyvalence est appréciée, tandis qu’une corrélation négative pourrait indiquer que la spécialisation mène à de meilleurs résultats. Une absence de corrélation significative signifierait que la diversité technologique n’est pas un facteur déterminant de la popularité.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Le nuage de points avec gradient de couleur permet de visualiser à la fois la relation entre diversité et popularité, ainsi que l’identification visuelle des profils les plus populaires (points rouges). La ligne de régression indique la tendance générale, mais les points individuels peuvent révéler des exceptions intéressantes. Par exemple, certains profils peuvent être très populaires avec peu de langages (spécialisation réussie) ou avec beaucoup de langages (polyvalence réussie).",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#identification-des-superstars",
    "href": "qmd/01-analyse-profiles-github.html#identification-des-superstars",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "9.1 Identification des superstars",
    "text": "9.1 Identification des superstars\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : La définition des “superstars” comme étant dans le top 25% permet de créer un groupe d’élite pour comparaison. Le quantile 0.75 (Q3) est un seuil objectif qui sépare naturellement les profils les plus performants. Environ 7-8 profils devraient être identifiés comme superstars, ce qui représente un échantillon suffisant pour des comparaisons statistiques significatives.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#comparaison-des-langages-pour-les-superstars",
    "href": "qmd/01-analyse-profiles-github.html#comparaison-des-langages-pour-les-superstars",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "9.2 Comparaison des langages pour les superstars",
    "text": "9.2 Comparaison des langages pour les superstars\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Cette analyse compare les préférences technologiques entre les profils les plus populaires et les autres. Si un langage apparaît significativement plus souvent chez les superstars, cela pourrait suggérer que ce langage offre des avantages (écosystème riche, communauté active, outils performants) ou que les superstars tendent à choisir ce langage pour des raisons stratégiques. Cependant, il faut éviter de conclure à une relation causale directe.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#visualisation-comparée",
    "href": "qmd/01-analyse-profiles-github.html#visualisation-comparée",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "9.3 Visualisation comparée",
    "text": "9.3 Visualisation comparée\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Le graphique en barres groupées permet une comparaison visuelle directe. Des barres de hauteur similaire indiquent que le langage est également utilisé dans les deux groupes, tandis que des différences importantes suggèrent une préférence marquée dans un groupe. Cette visualisation aide à identifier rapidement si certains langages sont des “marchés” des superstars ou si la répartition est homogène.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#résultats-principaux",
    "href": "qmd/01-analyse-profiles-github.html#résultats-principaux",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "12.1 Résultats principaux",
    "text": "12.1 Résultats principaux\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInterprétation : Cette synthèse quantitative résume les principales découvertes de l’analyse. La comparaison entre médiane et moyenne révèle le degré d’asymétrie : si la moyenne est beaucoup plus élevée que la médiane, cela confirme la présence de quelques profils extrêmement populaires qui tirent la moyenne vers le haut. Les statistiques sur les langages montrent leur prévalence relative dans l’échantillon. La comparaison organisations/individus quantifie l’écart de popularité moyen entre les deux groupes. Enfin, les métriques de diversité technologique et leur corrélation avec la popularité fournissent une mesure objective de l’importance de la polyvalence vs spécialisation.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#interprétation",
    "href": "qmd/01-analyse-profiles-github.html#interprétation",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "12.2 Interprétation",
    "text": "12.2 Interprétation\n\n12.2.1 1. Distribution de la popularité\nLa distribution des étoiles est fortement asymétrique, avec quelques profils exceptionnellement populaires (outliers) et une grande majorité de profils avec une popularité modérée. Cette distribution justifie l’utilisation d’échelles logarithmiques pour les visualisations.\n\n\n12.2.2 2. Impact des langages de programmation\nL’analyse révèle que :\n\nPython et JavaScript sont parmi les langages les plus représentés dans les profils populaires.\nLes tests statistiques (Wilcoxon) permettent de déterminer si l’utilisation d’un langage spécifique est associée à une popularité significativement plus élevée.\n\n\n\n12.2.3 3. Organisations vs Individus\nLes organisations majeures (OpenAI, GitHub, Microsoft, Google) présentent généralement des niveaux de popularité très élevés, mais cette différence doit être interprétée avec prudence compte tenu du faible nombre d’organisations dans l’échantillon.\n\n\n12.2.4 4. Diversité technologique\nL’analyse de corrélation entre le nombre de langages utilisés et la popularité permet d’évaluer si la polyvalence technologique est un facteur de succès.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#limitations",
    "href": "qmd/01-analyse-profiles-github.html#limitations",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "12.3 Limitations",
    "text": "12.3 Limitations\n\nTaille de l’échantillon : 30 profils est un échantillon relativement petit pour des généralisations statistiques.\nBiais de sélection : Les profils sélectionnés sont déjà parmi les plus populaires, ce qui peut créer un biais.\nFacteurs confondants : De nombreux autres facteurs (qualité du code, documentation, communauté, etc.) influencent la popularité et ne sont pas pris en compte dans cette analyse.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/01-analyse-profiles-github.html#perspectives",
    "href": "qmd/01-analyse-profiles-github.html#perspectives",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "12.4 Perspectives",
    "text": "12.4 Perspectives\nCette analyse pourrait être étendue en :\n\nAugmentant la taille de l’échantillon\nAjoutant des variables temporelles (évolution dans le temps)\nAnalysant d’autres métriques (forks, contributions, etc.)\nIntégrant des données sur la localisation géographique des contributeurs",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "index.html#méthodologie",
    "href": "index.html#méthodologie",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "2 Méthodologie",
    "text": "2 Méthodologie\nPour mener à bien cette analyse, nous utiliserons le langage de programmation R ainsi que les outils du Tidyverse. Notre démarche suivra les étapes classiques de la science des données :\n\nChargement et exploration des données - Compréhension de la structure du jeu de données\nNettoyage et transformation - Préparation des données pour l’analyse\nAnalyse descriptive - Statistiques descriptives et visualisations\nTests statistiques - Validation des hypothèses avec des tests appropriés\nInterprétation des résultats - Conclusion et synthèse"
  },
  {
    "objectID": "index.html#structure-du-projet",
    "href": "index.html#structure-du-projet",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "3 Structure du Projet",
    "text": "3 Structure du Projet\nCe site web contient :\n\nAnalyse Principale - Analyse complète des profils GitHub"
  },
  {
    "objectID": "index.html#technologies-utilisées",
    "href": "index.html#technologies-utilisées",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "4 Technologies Utilisées",
    "text": "4 Technologies Utilisées\n\nR - Langage de programmation statistique\nTidyverse - Collection de packages R pour la science des données\nggplot2 - Création de graphiques\ndplyr - Manipulation de données\nQuarto - Publication scientifique reproductible"
  },
  {
    "objectID": "index.html#données",
    "href": "index.html#données",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "5 Données",
    "text": "5 Données\nLe jeu de données profiles_index.csv contient les informations sur 30 profils GitHub incluant : - Nom d’utilisateur (login) et nom réel - Entreprise et localisation - Nombre total d’étoiles (total_stars) - Nombre de dépôts analysés - Liste des langages de programmation utilisés"
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#introduction",
    "href": "qmd/02-presentation-profiles-github.html#introduction",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Introduction",
    "text": "Introduction\nCe projet porte sur l’analyse statistique des profils GitHub les plus influents à travers le monde.\nGitHub étant la plateforme de référence pour le développement collaboratif, le nombre d’étoiles (stars) récoltées par un utilisateur est devenu un indicateur clé de sa popularité et de son impact dans la communauté “Open Source”.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#objectifs-de-létude",
    "href": "qmd/02-presentation-profiles-github.html#objectifs-de-létude",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Objectifs de l’étude",
    "text": "Objectifs de l’étude\nNous tenterons de répondre à plusieurs questions fondamentales :\n\nDistribution : Comment la popularité est-elle distribuée entre ces différents profils ?\nLangages : Le choix des langages de programmation influence-t-il le succès ?\nTypes : Existe-t-il des différences entre les comptes individuels et les organisations ?\nDiversité : La diversité technologique est-elle corrélée à la popularité ?",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#jeu-de-données",
    "href": "qmd/02-presentation-profiles-github.html#jeu-de-données",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Jeu de données",
    "text": "Jeu de données\n30 développeurs et organisations majeures :\n\nGoogle, Microsoft, OpenAI\nDéveloppeurs influents (Linus Torvalds, Evan You, etc.)\nVariables : étoiles, langages, localisation, entreprise\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#aperçu-des-données",
    "href": "qmd/02-presentation-profiles-github.html#aperçu-des-données",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Aperçu des données",
    "text": "Aperçu des données\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#nettoyage-des-données",
    "href": "qmd/02-presentation-profiles-github.html#nettoyage-des-données",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Nettoyage des données",
    "text": "Nettoyage des données\nRemplacement des valeurs manquantes\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#création-de-variables-pour-les-langages",
    "href": "qmd/02-presentation-profiles-github.html#création-de-variables-pour-les-langages",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Création de variables pour les langages",
    "text": "Création de variables pour les langages\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#statistiques-descriptives",
    "href": "qmd/02-presentation-profiles-github.html#statistiques-descriptives",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Statistiques descriptives",
    "text": "Statistiques descriptives\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#distribution-de-la-popularité",
    "href": "qmd/02-presentation-profiles-github.html#distribution-de-la-popularité",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Distribution de la popularité",
    "text": "Distribution de la popularité\nQuestion 1 : Comment la popularité est-elle distribuée ?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#observation-distribution-asymétrique",
    "href": "qmd/02-presentation-profiles-github.html#observation-distribution-asymétrique",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Observation : Distribution asymétrique",
    "text": "Observation : Distribution asymétrique\n\nQuelques profils exceptionnellement populaires (outliers)\nGrande majorité avec popularité modérée\nJustifie l’utilisation d’échelles logarithmiques pour les visualisations",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#impact-de-python",
    "href": "qmd/02-presentation-profiles-github.html#impact-de-python",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Impact de Python",
    "text": "Impact de Python\nQuestion 2 : Python influence-t-il le succès ?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#test-statistique-python",
    "href": "qmd/02-presentation-profiles-github.html#test-statistique-python",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Test statistique : Python",
    "text": "Test statistique : Python\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#impact-de-javascript",
    "href": "qmd/02-presentation-profiles-github.html#impact-de-javascript",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Impact de JavaScript",
    "text": "Impact de JavaScript\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#test-statistique-javascript",
    "href": "qmd/02-presentation-profiles-github.html#test-statistique-javascript",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Test statistique : JavaScript",
    "text": "Test statistique : JavaScript\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#organisations-vs-individus",
    "href": "qmd/02-presentation-profiles-github.html#organisations-vs-individus",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Organisations vs Individus",
    "text": "Organisations vs Individus\nQuestion 3 : Différences entre organisations et individus ?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#visualisation-organisations-vs-individus",
    "href": "qmd/02-presentation-profiles-github.html#visualisation-organisations-vs-individus",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Visualisation : Organisations vs Individus",
    "text": "Visualisation : Organisations vs Individus\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#relation-dépôts-vs-popularité",
    "href": "qmd/02-presentation-profiles-github.html#relation-dépôts-vs-popularité",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Relation : Dépôts vs Popularité",
    "text": "Relation : Dépôts vs Popularité\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#test-de-corrélation",
    "href": "qmd/02-presentation-profiles-github.html#test-de-corrélation",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Test de corrélation",
    "text": "Test de corrélation\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#diversité-technologique",
    "href": "qmd/02-presentation-profiles-github.html#diversité-technologique",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Diversité technologique",
    "text": "Diversité technologique\nQuestion 4 : La diversité est-elle corrélée à la popularité ?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#visualisation-diversité-vs-popularité",
    "href": "qmd/02-presentation-profiles-github.html#visualisation-diversité-vs-popularité",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Visualisation : Diversité vs Popularité",
    "text": "Visualisation : Diversité vs Popularité\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#corrélation-diversité-technologique",
    "href": "qmd/02-presentation-profiles-github.html#corrélation-diversité-technologique",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Corrélation : Diversité technologique",
    "text": "Corrélation : Diversité technologique\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#identification-des-superstars-top-25",
    "href": "qmd/02-presentation-profiles-github.html#identification-des-superstars-top-25",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Identification des Superstars (Top 25%)",
    "text": "Identification des Superstars (Top 25%)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#comparaison-langages-des-superstars",
    "href": "qmd/02-presentation-profiles-github.html#comparaison-langages-des-superstars",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Comparaison : Langages des Superstars",
    "text": "Comparaison : Langages des Superstars\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#visualisation-langages-des-superstars",
    "href": "qmd/02-presentation-profiles-github.html#visualisation-langages-des-superstars",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Visualisation : Langages des Superstars",
    "text": "Visualisation : Langages des Superstars\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#détection-des-valeurs-atypiques-outliers",
    "href": "qmd/02-presentation-profiles-github.html#détection-des-valeurs-atypiques-outliers",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Détection des valeurs atypiques (Outliers)",
    "text": "Détection des valeurs atypiques (Outliers)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#visualisation-des-outliers",
    "href": "qmd/02-presentation-profiles-github.html#visualisation-des-outliers",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Visualisation des Outliers",
    "text": "Visualisation des Outliers\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#nuage-de-mots-entreprises",
    "href": "qmd/02-presentation-profiles-github.html#nuage-de-mots-entreprises",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Nuage de mots : Entreprises",
    "text": "Nuage de mots : Entreprises\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#synthèse-des-résultats",
    "href": "qmd/02-presentation-profiles-github.html#synthèse-des-résultats",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Synthèse des résultats",
    "text": "Synthèse des résultats\n1. Distribution de la popularité\n\nDistribution fortement asymétrique\nMédiane : r round(median(df$total_stars), 0) étoiles\nQuelques profils exceptionnels (outliers)\n\n2. Impact des langages\n\nPython et JavaScript sont très représentés\nTests statistiques permettent d’évaluer l’impact\n\n3. Organisations vs Individus\n\nOrganisations majeures très populaires\nInterprétation prudente (petit échantillon)",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#synthèse-des-résultats-suite",
    "href": "qmd/02-presentation-profiles-github.html#synthèse-des-résultats-suite",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Synthèse des résultats (suite)",
    "text": "Synthèse des résultats (suite)\n4. Diversité technologique\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#limitations",
    "href": "qmd/02-presentation-profiles-github.html#limitations",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Limitations",
    "text": "Limitations\n\nTaille de l’échantillon : 30 profils est relativement petit\nBiais de sélection : Profils déjà parmi les plus populaires\nFacteurs confondants :\n\nQualité du code\nDocumentation\nCommunauté\nTiming et visibilité",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#perspectives-dextension",
    "href": "qmd/02-presentation-profiles-github.html#perspectives-dextension",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Perspectives d’extension",
    "text": "Perspectives d’extension\n\nAugmenter l’échantillon : Analyser plus de profils\nVariables temporelles : Évolution dans le temps\nAutres métriques : Forks, contributions, issues\nAnalyse géographique : Impact de la localisation\nAnalyse de réseaux : Relations entre contributeurs",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#conclusion",
    "href": "qmd/02-presentation-profiles-github.html#conclusion",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Conclusion",
    "text": "Conclusion\nCette analyse révèle des patterns intéressants sur les facteurs associés à la popularité sur GitHub :\n✓ Distribution asymétrique de la popularité\n✓ Rôle des langages de programmation\n✓ Différences organisations/individus\n✓ Importance de considérer plusieurs facteurs simultanément",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#questions",
    "href": "qmd/02-presentation-profiles-github.html#questions",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Questions ?",
    "text": "Questions ?\nMéthodologie : R + Tidyverse + Tests statistiques\nDonnées : 30 profils GitHub influents\nRésultats : Visualisations + Tests statistiques + Interprétations",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#références",
    "href": "qmd/02-presentation-profiles-github.html#références",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Références",
    "text": "Références\n\nWickham, H., & Grolemund, G. (2016). R for Data Science. O’Reilly Media.\nWickham, H., et al. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686.\nR Core Team (2023). R: A language and environment for statistical computing.",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  },
  {
    "objectID": "qmd/02-presentation-profiles-github.html#fin-de-la-présentation",
    "href": "qmd/02-presentation-profiles-github.html#fin-de-la-présentation",
    "title": "Analyse Statistique des Profils GitHub Influents",
    "section": "Fin de la présentation",
    "text": "Fin de la présentation\nMerci pour votre attention !\nAnalyse réalisée avec R, Tidyverse et Quarto",
    "crumbs": [
      "Projet Principal",
      "Analyse Statistique des Profils GitHub Influents"
    ]
  }
]