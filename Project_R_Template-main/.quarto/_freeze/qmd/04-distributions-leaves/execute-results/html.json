{
  "hash": "3699d2748dbac100fe8ef39144797a9b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"04-Distribution and Confidence Intervals of Maple Leaf Samples\"\ndate: '2025-11-04'\n---\n\n\n\n<!--\nleaves <- read.csv(\"https://raw.githubusercontent.com/tpetzoldt/datasets/main/data/leaves.csv\")\n--->\n\n\n\n# Introduction\n\nThe example aims to demonstrate estimation and interpretation of prediction intervals and confidence intervals.\nAt the end, the two samples are compared with respect to variance and mean values.\n\nThe experimental hypotheses is, that the sampling strategy has an influence on the parameters of the distribution, i.e. that a sampling bias may occur.\nHere we leave it open, if the \"subjective sampling\" strategy prefers bigger or smaller leaves or if it has an influence on variance.\nThe result is to be visualized with bar charts and box plots.\nWe use the **leave width** as an example, an analysis of the other variables is left as an optional exercise.\n\nWe can now derive the following statistical hypotheses **about the variance:**\n\n* $H_0$: The variance of both samples is the same.\n* $H_a$: The samples have different variance.\n\n**and about the mean:**\n\n* $H_0$: The mean of both samples is the same.\n* $H_a$: The mean values of the samples are different.\n\n# Material and Methods\n\nThe data set consists of two samples of maple leaves (*Acer platanoides*), \nsampled in front of the institute building (Fig. 1).\n\n![Fig 1.: Maple leaves in front of the institute](../img/leaves.jpg){width=50%}\n\nThe two samples were collected with different sampling strategies:\n\n * HYB: hydrobiology group, got random sample from the supervisor\n * HSE: hydroscience group, had the freedom to collect their leaves themselves\n\nThen length, width and stalk length were measured in millimeters with a ruler \n(Fig. 2) and the data collected in a spreadsheet table and a csv-file.\n\n![Fig 2.: Sample measures of maple leaves. Note that the stalk length does not include length of the leaf blade.](../img/leaves_measures.png){width=25%}\n\n\nThe statistical analysis is performed with the **R** software for statistical \ncomputing and graphics [@RCore].\n\n# Data Analysis\n\n## Prepare and inspect data\n\nThe data set is available from your local learning management system (LMS,\ne.g. OPAL at TU Dresden) or publicly from\n[https://tpetzoldt.github.io/datasets/data/leaves.csv](https://tpetzoldt.github.io/datasets/data/leaves.csv).\n\n* Download the data set `leaves.csv` and use one of RStudio's \"Import Dataset\" wizards.\n* Alternative: use `read.csv()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#  ... do it\n```\n:::\n\n\n* plot everything, just for testing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(leaves)\n```\n:::\n\n\n* First, let's apply a traditional approach and split `leaves` in two separate tables for the samples HSE and MHYB:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhyb <- subset(leaves, group == \"HYB\")\nhse <- subset(leaves, group == \"HSE\")\n```\n:::\n\n\n* Then, compare leaf **width** of both groups graphically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxplot(hse$width, hyb$width, names=c(\"HSE\", \"HYB\"))\n```\n:::\n\n\n## Check distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use `hist`, `qqnorm`, `qqline`\n# ...\n```\n:::\n\n\n## Sample statistics and prediction interval\n\nIn a first analysis, we want to estimate the interval that covers 95% of leaves, defined by their width.\nAs a first method, we take the **empirical quantiles** directly from the data. The method is also called \"**nonparametric**,\" because we don't calculate mean and standard deviation and do not assume a normal or any other distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(hse$width, p = c(0.025, 0.975))\n```\n:::\n\n\nNow, we compare this empirical result with a method that relies on a specific distribution.\nIf our initial graphical visualization (e.g., the histogram) suggests the data is reasonably symmetric and bell-shaped, we can proceed with a parametric assumption.\n\nWe first calculate mean, sd, N and se for \"hse\" data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhse.mean <- mean(hse$width)\nhse.sd   <- sd(hse$width)\nhse.N    <- length(hse$width)\nhse.se   <- hse.sd/sqrt(hse.N)\n```\n:::\n\n\nThen we estimate an approximate two-sided 95% **prediction interval** ($PI$) for the sample using a simplified approach based on the quantiles of the theoretical normal distribution ($z_{\\alpha/2} \\approx 1.96$) and the sample parameters mean $\\bar{x}$ and standard deviation ($s$):\n\n$$\nPI = \\bar{x} \\pm z \\cdot s\n$$\n\nThis is the interval where we would predict a new single observation to fall with 95% confidence.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhse.95 <- hse.mean + c(-1.96, 1.96) * hse.sd\nhse.95\n```\n:::\n\n\nInstead of using 1.96, we could also use the quantile function `qnorm(0.975)` for the upper interval or `qnorm(c(0.025, 0.975))` for the lower and upper in parallel:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhse.95 <- hse.mean + qnorm(c(0.025, 0.975)) * hse.sd\nhse.95\n```\n:::\n\n\nNow we plot the data and indicate the 95% interval:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hse$width)\nabline(h = hse.95, col=\"red\")\n```\n:::\n\n\n... and the same as histogram:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(hse$width)\nabline(v = hse.95, col=\"red\")\nrug(hse$width, col=\"blue\")\n```\n:::\n\n\n\n## Confidence interval of the mean\n\nThe **confidence interval** ($CI$) of the mean tells us how precise a mean value was estimated from data.\nIf the sample size is \"large enough\", the distribution of the raw data does not necessarily need to be normal, because then mean values tend to approximate a normal distribution due to the **central limit theorem**.\n\nThe formula for the CI of the mean is: $$CI = \\bar{x} \\pm t_{\\alpha/2, n-1} \\cdot \\frac{s}{\\sqrt{N}}$$\n\n### Confidence interval of the mean for the \"hse\" data\n\n**Task:** Calculate the confidence interval of the mean value of the \"hse\" data set using the quantile function (`qt`) of the t-distribution^[as the sample size is not too small, you may also compare this with 1.96 or 2.0]:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhse.ci <- hse.mean + qt(p = c(0.025, 0.975), df = hse.N - 1) * hse.se\n```\n:::\n\n\nNow indicate the confidence interval of the mean in the histogram.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nabline(v = hse.ci, col=\"magenta\")\n```\n:::\n\n\n### Confidence interval for the mean of the \"hyb\" data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#  Do the same for the \"hyb\" data, calculate mean, sd, N, se and ci.\n# ...\n```\n:::\n\n\n### Discussion: Comparison and interpretation\n\nExplain the fundamental statistical reason why the 95% Prediction Interval (PI) for the leaf width is always significantly wider than the 95% Confidence Interval (CI) for the mean leaf width, even though both intervals are calculated from the same data set (hse).\n\n\n## Comparison of the samples\n\nTo compare the two samples. we already created box plots at the beginning.\nInstead of a boxplot, we can also use a bar chart with confidence intervals.  \nThis can be done with the add-on package **gplots** (not to be confused with **ggplot**):\n\n**Solution A) with package gplots**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"gplots\")\nbarplot2(height = c(hyb.mean, hse.mean),\n         ci.l   = c(hyb.ci[1], hse.ci[1]),\n         ci.u   = c(hyb.ci[2], hse.ci[2]),\n         plot.ci = TRUE,\n         names.arg=c(\"Hyb\", \"HSE\")\n)\n```\n:::\n\n\n**Solution B) without add-on packages (optional)**\n\nHere we use a standard bar chart, and line segments for the error bars.\nOne small problem arises, because `barplot` creates an own x-scaling.\nThe good news is, that `barplot` returns its x-scale.\nWe can store it in a variable, e.g. `x` that can then be used in subsequent code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- barplot(c(hyb.mean, hse.mean),\n  names.arg=c(\"HYB\", \"HSE\"), ylim=c(0, 150))\nsegments(x0=x[1], y0=hyb.ci[1], y1=hyb.ci[2], lwd=2)\nsegments(x0=x[2], y0=hse.ci[1], y1=hse.ci[2], lwd=2)\n```\n:::\n\n\n\n## Is the difference between the samples statistically significant?\n\nIn the following, we compare the two samples with t- and F-Tests.\n\n**Hypotheses:**\n\n**$H_0$**: Both samples have the same mean width and variance.\n\n**$H_A$:** The mean width (and possibly also the variance) differ because of more subjective sampling of HSE students.\n  They may have prefered bigger or the nice small leaves.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(width ~ group, data = leaves)\n```\n:::\n\n\nPerform also the classical t-test (`var.equal=TRUE`) and the F-test (`var.test`).\nCalculate absolute and relative effect size (mean differences) and interpret the results of all 3 tests.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# var.test(...)\n# t.test(...)\n# ...\n```\n:::\n\n\n# Appendix\n\nThe following is optional for all who feel underchallenged or just want to learn more.\n\n## Calculation of summary statistics with **dplyr**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"dplyr\")\nleaves <- read.csv(\"leaves.csv\")\n\nstats <-\n  leaves |>\n    group_by(group) |>\n    summarize(mean = mean(width), sd = sd(width),\n              N = length(width), se = sd/sqrt(N),\n              lwr = mean + qt(p = 0.025, df = N-1) * se,\n              upr = mean + qt(p = 0.975, df = N-1) * se\n             )\n\nstats\n```\n:::\n\n\n## Barchart and errorbars with **ggplot2**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"ggplot2\")\nstats |>\n  ggplot(aes(x=group, y=mean, min=lwr, max=upr))  +\n    geom_col() + geom_errorbar(width=0.2)\n```\n:::\n\n\n##  A footnote about prediction intervals\n\nThe simplified $\\bar{x} \\pm z \\cdot s$ formula used above is an approximation. A statistically rigorous 95% prediction interval, especially for smaller samples, needs two corrections.\n\nFirst, we would use the **t-distribution** with the quantile $t_{\\alpha/2, n-1}$ (or `qt(alpha/2, n-1)` in R) instead of the normal quantiles ($\\pm 1.96$). Then we add a term $\\sqrt{1+1/N}$ that corrects for the sample parameters. The full formula for a single future observation is then:\n\n$$\\text{PI} = \\bar{x} \\pm t_{\\alpha/2, n-1} \\cdot s \\cdot \\sqrt{1 + \\frac{1}{N}}$$\n\nThe prediction interval is related to the so-called \"tolerance interval\". Both are the same if the population parameters $\\mu, \\sigma$ are known or the sample size is very big. However, there are theoretical and practical differences in case of small sample size.\n\n\n\n# References\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}